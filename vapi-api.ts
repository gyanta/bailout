/**
 * Generated by orval v6.29.1 üç∫
 * Do not edit manually.
 * Vapi API
 * API for building voice assistants
 * OpenAPI spec version: 1.0
 */
import axios from 'axios'
import type {
  AxiosRequestConfig,
  AxiosResponse
} from 'axios'
export type LoggingControllerGetLogsSortOrder = typeof LoggingControllerGetLogsSortOrder[keyof typeof LoggingControllerGetLogsSortOrder];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const LoggingControllerGetLogsSortOrder = {
  ASC: 'ASC',
  DESC: 'DESC',
} as const;

export type LoggingControllerGetLogsParams = {
callId: string;
/**
 * This is the page number to return. Defaults to 1.
 */
page?: number;
/**
 * This is the sort order for pagination. Defaults to 'ASC'.
 */
sortOrder?: LoggingControllerGetLogsSortOrder;
/**
 * This is the maximum number of items to return. Defaults to 100.
 */
limit?: number;
/**
 * This will return items where the createdAt is greater than the specified value.
 */
createdAtGt?: string;
/**
 * This will return items where the createdAt is less than the specified value.
 */
createdAtLt?: string;
/**
 * This will return items where the createdAt is greater than or equal to the specified value.
 */
createdAtGe?: string;
/**
 * This will return items where the createdAt is less than or equal to the specified value.
 */
createdAtLe?: string;
/**
 * This will return items where the updatedAt is greater than the specified value.
 */
updatedAtGt?: string;
/**
 * This will return items where the updatedAt is less than the specified value.
 */
updatedAtLt?: string;
/**
 * This will return items where the updatedAt is greater than or equal to the specified value.
 */
updatedAtGe?: string;
/**
 * This will return items where the updatedAt is less than or equal to the specified value.
 */
updatedAtLe?: string;
};

export type VoiceLibraryControllerVoiceLibrarySyncByProvider201 = { [key: string]: any };

export type MetricsControllerFindAllParams = {
/**
 * Convert date & and time to provided timezone. https://popsql.com/learn-sql/postgresql/how-to-convert-utc-to-local-time-zone-in-postgresql
 */
timezone?: string;
/**
 * This will include calls with a createdAt timestamp greater than or equal to the specified value.

If not provided, defaults to the org's current period start.
 */
rangeStart?: string;
/**
 * This will include calls with a createdAt timestamp less than the specified value.

If not provided, the default value will be the current timestamp.
 */
rangeEnd?: string;
};

export type PhoneNumberControllerFindAllParams = {
/**
 * This is the maximum number of items to return. Defaults to 100.
 */
limit?: number;
/**
 * This will return items where the createdAt is greater than the specified value.
 */
createdAtGt?: string;
/**
 * This will return items where the createdAt is less than the specified value.
 */
createdAtLt?: string;
/**
 * This will return items where the createdAt is greater than or equal to the specified value.
 */
createdAtGe?: string;
/**
 * This will return items where the createdAt is less than or equal to the specified value.
 */
createdAtLe?: string;
/**
 * This will return items where the updatedAt is greater than the specified value.
 */
updatedAtGt?: string;
/**
 * This will return items where the updatedAt is less than the specified value.
 */
updatedAtLt?: string;
/**
 * This will return items where the updatedAt is greater than or equal to the specified value.
 */
updatedAtGe?: string;
/**
 * This will return items where the updatedAt is less than or equal to the specified value.
 */
updatedAtLe?: string;
};

export type OrgControllerUserInvite201 = { [key: string]: any };

export type CredentialControllerRemove200 = TwilioCredential | DeepgramCredential | OpenAICredential | TogetherAICredential | AnyscaleCredential | OpenRouterCredential | PerplexityAICredential | DeepInfraCredential | CustomLLMCredential | ElevenLabsCredential | PlayHTCredential | RimeAICredential | UpdateRunpodCredentialDTO | UpdateGroqCredentialDTO | UpdateAnthropicCredentialDTO | UpdateVonageCredentialDTO;

export type CredentialControllerUpdate200 = TwilioCredential | DeepgramCredential | OpenAICredential | TogetherAICredential | AnyscaleCredential | OpenRouterCredential | PerplexityAICredential | DeepInfraCredential | CustomLLMCredential | ElevenLabsCredential | PlayHTCredential | RimeAICredential | UpdateRunpodCredentialDTO | UpdateGroqCredentialDTO | UpdateAnthropicCredentialDTO | UpdateVonageCredentialDTO;

export type CredentialControllerUpdateBody = UpdateTwilioCredentialDTO | UpdateDeepgramCredentialDTO | UpdateOpenAICredentialDTO | UpdateTogetherAICredentialDTO | UpdateAnyscaleCredentialDTO | UpdateOpenRouterCredentialDTO | UpdatePerplexityAICredentialDTO | UpdateDeepInfraCredentialDTO | UpdateCustomLLMCredentialDTO | UpdateElevenLabsCredentialDTO | UpdatePlayHTCredentialDTO | UpdateRimeAICredentialDTO | UpdateRunpodCredentialDTO | UpdateGroqCredentialDTO | UpdateAnthropicCredentialDTO | UpdateVonageCredentialDTO;

export type CredentialControllerFindOne200 = TwilioCredential | DeepgramCredential | OpenAICredential | TogetherAICredential | AnyscaleCredential | OpenRouterCredential | PerplexityAICredential | DeepInfraCredential | CustomLLMCredential | ElevenLabsCredential | PlayHTCredential | RimeAICredential | UpdateRunpodCredentialDTO | UpdateGroqCredentialDTO | UpdateAnthropicCredentialDTO | UpdateVonageCredentialDTO;

export type CredentialControllerFindAll200Item = TwilioCredential | DeepgramCredential | OpenAICredential | TogetherAICredential | AnyscaleCredential | OpenRouterCredential | PerplexityAICredential | DeepInfraCredential | CustomLLMCredential | ElevenLabsCredential | PlayHTCredential | RimeAICredential | UpdateRunpodCredentialDTO | UpdateGroqCredentialDTO | UpdateAnthropicCredentialDTO | UpdateVonageCredentialDTO;

export type CredentialControllerFindAllParams = {
/**
 * This is the maximum number of items to return. Defaults to 100.
 */
limit?: number;
/**
 * This will return items where the createdAt is greater than the specified value.
 */
createdAtGt?: string;
/**
 * This will return items where the createdAt is less than the specified value.
 */
createdAtLt?: string;
/**
 * This will return items where the createdAt is greater than or equal to the specified value.
 */
createdAtGe?: string;
/**
 * This will return items where the createdAt is less than or equal to the specified value.
 */
createdAtLe?: string;
/**
 * This will return items where the updatedAt is greater than the specified value.
 */
updatedAtGt?: string;
/**
 * This will return items where the updatedAt is less than the specified value.
 */
updatedAtLt?: string;
/**
 * This will return items where the updatedAt is greater than or equal to the specified value.
 */
updatedAtGe?: string;
/**
 * This will return items where the updatedAt is less than or equal to the specified value.
 */
updatedAtLe?: string;
};

export type CredentialControllerCreate201 = TwilioCredential | DeepgramCredential | OpenAICredential | TogetherAICredential | AnyscaleCredential | OpenRouterCredential | PerplexityAICredential | DeepInfraCredential | CustomLLMCredential | ElevenLabsCredential | PlayHTCredential | RimeAICredential | UpdateRunpodCredentialDTO | UpdateGroqCredentialDTO | UpdateAnthropicCredentialDTO | UpdateVonageCredentialDTO;

export type CredentialControllerCreateBody = CreateTwilioCredentialDTO | CreateDeepgramCredentialDTO | CreateOpenAICredentialDTO | CreateTogetherAICredentialDTO | CreateAnyscaleCredentialDTO | CreateOpenRouterCredentialDTO | CreatePerplexityAICredentialDTO | CreateDeepInfraCredentialDTO | CreateCustomLLMCredentialDTO | CreateElevenLabsCredentialDTO | CreatePlayHTCredentialDTO | CreateRimeAICredentialDTO | CreateRunpodCredentialDTO | CreateGroqCredentialDTO | CreateAnthropicCredentialDTO | CreateVonageCredentialDTO;

export type CallControllerFindAllPaginatedSortOrder = typeof CallControllerFindAllPaginatedSortOrder[keyof typeof CallControllerFindAllPaginatedSortOrder];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CallControllerFindAllPaginatedSortOrder = {
  ASC: 'ASC',
  DESC: 'DESC',
} as const;

export type CallControllerFindAllPaginatedParams = {
/**
 * This will return calls with the specified assistantId.
 */
assistantId?: string;
/**
 * This is the page number to return. Defaults to 1.
 */
page?: number;
/**
 * This is the sort order for pagination. Defaults to 'ASC'.
 */
sortOrder?: CallControllerFindAllPaginatedSortOrder;
/**
 * This is the maximum number of items to return. Defaults to 100.
 */
limit?: number;
/**
 * This will return items where the createdAt is greater than the specified value.
 */
createdAtGt?: string;
/**
 * This will return items where the createdAt is less than the specified value.
 */
createdAtLt?: string;
/**
 * This will return items where the createdAt is greater than or equal to the specified value.
 */
createdAtGe?: string;
/**
 * This will return items where the createdAt is less than or equal to the specified value.
 */
createdAtLe?: string;
/**
 * This will return items where the updatedAt is greater than the specified value.
 */
updatedAtGt?: string;
/**
 * This will return items where the updatedAt is less than the specified value.
 */
updatedAtLt?: string;
/**
 * This will return items where the updatedAt is greater than or equal to the specified value.
 */
updatedAtGe?: string;
/**
 * This will return items where the updatedAt is less than or equal to the specified value.
 */
updatedAtLe?: string;
};

export type CallControllerFindAllParams = {
/**
 * This will return calls with the specified assistantId.
 */
assistantId?: string;
/**
 * This is the maximum number of items to return. Defaults to 100.
 */
limit?: number;
/**
 * This will return items where the createdAt is greater than the specified value.
 */
createdAtGt?: string;
/**
 * This will return items where the createdAt is less than the specified value.
 */
createdAtLt?: string;
/**
 * This will return items where the createdAt is greater than or equal to the specified value.
 */
createdAtGe?: string;
/**
 * This will return items where the createdAt is less than or equal to the specified value.
 */
createdAtLe?: string;
/**
 * This will return items where the updatedAt is greater than the specified value.
 */
updatedAtGt?: string;
/**
 * This will return items where the updatedAt is less than the specified value.
 */
updatedAtLt?: string;
/**
 * This will return items where the updatedAt is greater than or equal to the specified value.
 */
updatedAtGe?: string;
/**
 * This will return items where the updatedAt is less than or equal to the specified value.
 */
updatedAtLe?: string;
};

export type AssistantControllerFindAllParams = {
/**
 * This is the maximum number of items to return. Defaults to 100.
 */
limit?: number;
/**
 * This will return items where the createdAt is greater than the specified value.
 */
createdAtGt?: string;
/**
 * This will return items where the createdAt is less than the specified value.
 */
createdAtLt?: string;
/**
 * This will return items where the createdAt is greater than or equal to the specified value.
 */
createdAtGe?: string;
/**
 * This will return items where the createdAt is less than or equal to the specified value.
 */
createdAtLe?: string;
/**
 * This will return items where the updatedAt is greater than the specified value.
 */
updatedAtGt?: string;
/**
 * This will return items where the updatedAt is less than the specified value.
 */
updatedAtLt?: string;
/**
 * This will return items where the updatedAt is greater than or equal to the specified value.
 */
updatedAtGe?: string;
/**
 * This will return items where the updatedAt is less than or equal to the specified value.
 */
updatedAtLe?: string;
};

export type FileStatus = typeof FileStatus[keyof typeof FileStatus];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const FileStatus = {
  indexed: 'indexed',
  not_indexed: 'not_indexed',
} as const;

export type FileObject = typeof FileObject[keyof typeof FileObject];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const FileObject = {
  file: 'file',
} as const;

export type FileMetadata = { [key: string]: any };

export interface File {
  bucket?: string;
  bytes?: number;
  /** This is the ISO 8601 date-time string of when the file was created. */
  createdAt: string;
  /** This is the unique identifier for the file. */
  id: string;
  key?: string;
  metadata?: FileMetadata;
  mimetype?: string;
  /** This is the name of the file. This is just for your own reference. */
  name?: string;
  object?: FileObject;
  /** This is the unique identifier for the org that this file belongs to. */
  orgId: string;
  originalName?: string;
  path?: string;
  purpose?: string;
  status?: FileStatus;
  /** This is the ISO 8601 date-time string of when the file was last updated. */
  updatedAt: string;
  url?: string;
}

export interface CreateFileDTO {
  file: Blob;
}

/**
 * This is the level of the log message.
 */
export type CallLogPrivilegedLevel = typeof CallLogPrivilegedLevel[keyof typeof CallLogPrivilegedLevel];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CallLogPrivilegedLevel = {
  INFO: 'INFO',
  LOG: 'LOG',
  WARN: 'WARN',
  ERROR: 'ERROR',
  CHECKPOINT: 'CHECKPOINT',
} as const;

export interface CallLogPrivileged {
  /** This is the unique identifier for the call. */
  callId: string;
  /** This is the level of the log message. */
  level: CallLogPrivilegedLevel;
  /** This is the log message associated with the call. */
  log: string;
  /** This is the unique identifier for the org that this call log belongs to. */
  orgId: string;
  /** This is the ISO 8601 date-time string of when the log was created. */
  time: string;
}

export interface CallLogsPaginatedResponse {
  metadata: PaginationMeta;
  results: CallLogPrivileged[];
}

/**
 * This is the voice provider that will be used.
 */
export type VoiceLibraryProvider = typeof VoiceLibraryProvider[keyof typeof VoiceLibraryProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const VoiceLibraryProvider = {
  '11labs': '11labs',
  playht: 'playht',
  'rime-ai': 'rime-ai',
  deepgram: 'deepgram',
  openai: 'openai',
  azure: 'azure',
  lmnt: 'lmnt',
  neets: 'neets',
} as const;

/**
 * The gender of the voice.
 */
export type VoiceLibraryGender = typeof VoiceLibraryGender[keyof typeof VoiceLibraryGender];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const VoiceLibraryGender = {
  male: 'male',
  female: 'female',
} as const;

export interface VoiceLibrary {
  /** The accent of the voice. */
  accent?: string;
  /** The ISO 8601 date-time string of when the voice library was created. */
  createdAt: string;
  /** The credential ID of the voice. */
  credentialId?: string;
  /** The gender of the voice. */
  gender?: VoiceLibraryGender;
  /** The unique identifier for the voice library. */
  id: string;
  /** The deletion status of the voice. */
  isDeleted: boolean;
  /** The Public voice is shared accross all the organizations. */
  isPublic: boolean;
  /** The language of the voice. */
  language?: string;
  /** The language code of the voice. */
  languageCode?: string;
  /** The model of the voice. */
  model?: string;
  /** The name of the voice. */
  name?: string;
  /** The unique identifier for the organization that this voice library belongs to. */
  orgId: string;
  /** The preview URL of the voice. */
  previewUrl?: string;
  /** This is the voice provider that will be used. */
  provider?: VoiceLibraryProvider;
  /** The ID of the voice provided by the provider. */
  providerId?: string;
  /** The unique slug of the voice. */
  slug?: string;
  /** The supported models of the voice. */
  supportedModels?: string;
  /** The ISO 8601 date-time string of when the voice library was last updated. */
  updatedAt: string;
}

export type MetricsCallMinutesDailyBreakdown = { [key: string]: any };

export type MetricsCallMinutesAverageDailyBreakdown = { [key: string]: any };

export type MetricsCallCountDailyBreakdown = { [key: string]: any };

export type MetricsBillDailyBreakdown = { [key: string]: any };

export interface Metrics {
  bill: number;
  billDailyBreakdown: MetricsBillDailyBreakdown;
  billWithinBillingLimit: boolean;
  callActive: string;
  callActiveWithinConcurrencyLimit: boolean;
  callCount: string;
  callCountDailyBreakdown: MetricsCallCountDailyBreakdown;
  callMinutes: string;
  callMinutesAverage: string;
  callMinutesAverageDailyBreakdown: MetricsCallMinutesAverageDailyBreakdown;
  callMinutesDailyBreakdown: MetricsCallMinutesDailyBreakdown;
  orgId: string;
  rangeEnd: string;
  rangeStart: string;
}

export interface UpdatePhoneNumberDTO {
  /**
   * This is the assistant that will be used for incoming calls to this phone number.

If this is not set, then the phone number will not handle incoming calls.
   * @nullable
   */
  assistantId?: string | null;
  /**
   * This is the name of the phone number. This is just for your own reference.
   * @maxLength 40
   */
  name?: string;
  /** This is the server URL that will be used to handle this phone number.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.

This overrides the serverUrl set on the org. Order of precedence: function.serverUrl > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.

Same precendence logic as serverUrl. */
  serverUrlSecret?: string;
}

export interface ImportVonagePhoneNumberDTO {
  /**
   * This is the assistant that will be used for incoming calls to this phone number.

If this is not set, then the phone number will not handle incoming calls.
   * @nullable
   */
  assistantId?: string | null;
  /** This is the credential that will be used to handle this phone number.

You can add the Vonage credentials in the Provider Keys page on the dashboard to get the credentialId. */
  credentialId: string;
  /**
   * This is the name of the phone number. This is just for your own reference.
   * @maxLength 40
   */
  name?: string;
  /** This is the server URL that will be used to handle this phone number.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.

This overrides the serverUrl set on the org. Order of precedence: function.serverUrl > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.

Same precendence logic as serverUrl. */
  serverUrlSecret?: string;
  /** These are the digits of the phone number you own on your Vonage. */
  vonagePhoneNumber: string;
}

export interface PhoneNumber {
  /**
   * This is the assistant that will be used for incoming calls to this phone number.

If this is not set, then the phone number will not handle incoming calls.
   * @nullable
   */
  assistantId?: string | null;
  /** This is the ISO 8601 date-time string of when the phone number was created. */
  createdAt: string;
  /** This is the credential that will be used to handle this phone number.

This is for numbers not bought on Vapi. */
  credentialId?: string;
  /** This is the unique identifier for the phone number. */
  id: string;
  /**
   * This is the name of the phone number. This is just for your own reference.
   * @maxLength 40
   */
  name?: string;
  /** These are the digits of the phone number. */
  number: string;
  /** This is the unique identifier for the org that this phone number belongs to. */
  orgId: string;
  /** This is the server URL that will be used to handle this phone number.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.

This overrides the serverUrl set on the org. Order of precedence: function.serverUrl > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.

Same precendence logic as serverUrl. */
  serverUrlSecret?: string;
  /** This is the subscription's current period start. */
  stripeSubscriptionCurrentPeriodStart?: string;
  /** This is the subscription for the phone number. */
  stripeSubscriptionId?: string;
  /** This is the subscription's status. */
  stripeSubscriptionStatus?: string;
  /** This is the Twilio Account SID for the phone number.

This is for numbers not bought on Vapi. */
  twilioAccountSid?: string;
  /** This is the Twilio Auth Token for the phone number.

This is for numbers not bought on Vapi. */
  twilioAuthToken?: string;
  /** This is the ISO 8601 date-time string of when the phone number was last updated. */
  updatedAt: string;
}

export interface BuyPhoneNumberDTO {
  /**
   * This is the area code of the phone number to purchase.
   * @minLength 3
   * @maxLength 3
   */
  areaCode: string;
  /**
   * This is the assistant that will be used for incoming calls to this phone number.

If this is not set, then the phone number will not handle incoming calls.
   * @nullable
   */
  assistantId?: string | null;
  /**
   * This is the name of the phone number. This is just for your own reference.
   * @maxLength 40
   */
  name?: string;
  /** This is the server URL that will be used to handle this phone number.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.

This overrides the serverUrl set on the org. Order of precedence: function.serverUrl > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.

Same precendence logic as serverUrl. */
  serverUrlSecret?: string;
}

export interface InviteUserDTO {
  email: string;
}

export interface UpdateOrgDTO {
  /**
   * This is the monthly billing limit for the org. To go beyond $1000/mo, please contact us at support@vapi.ai.
   * @minimum 0
   * @maximum 1000
   */
  billingLimit?: number;
  /**
   * This is the concurrency limit for the org. This is the maximum number of calls that can be active at any given time. To go beyond 10, please contact us at support@vapi.ai.
   * @minimum 1
   * @maximum 10
   */
  concurrencyLimit?: number;
  /** When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false.
When HIPAA is enabled, only OpenAI/Custom LLM or Azure Providers will be available for LLM and Voice respectively.
This is due to the compliance requirements of HIPAA. Other providers may not meet these requirements. */
  hipaaEnabled?: boolean;
  /**
   * This is the name of the org. This is just for your own reference.
   * @maxLength 40
   */
  name?: string;
  /** This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation. */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret. */
  serverUrlSecret?: string;
}

export interface User {
  /** This is the ISO 8601 date-time string of when the profile was created. */
  createdAt: string;
  /** This is the email of the user that is associated with the profile. */
  email: string;
  /** This is the full name of the user that is associated with the profile. */
  fullName?: string;
  /** This is the unique identifier for the profile or user. */
  id: string;
  /** This is the ISO 8601 date-time string of when the profile was last updated. */
  updatedAt: string;
}

export interface Org {
  /**
   * This is the monthly billing limit for the org. To go beyond $1000/mo, please contact us at support@vapi.ai.
   * @minimum 0
   * @maximum 1000
   */
  billingLimit?: number;
  /**
   * This is the concurrency limit for the org. This is the maximum number of calls that can be active at any given time. To go beyond 10, please contact us at support@vapi.ai.
   * @minimum 1
   * @maximum 10
   */
  concurrencyLimit?: number;
  /** This is the ISO 8601 date-time string of when the org was created. */
  createdAt: string;
  /** When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false.
When HIPAA is enabled, only OpenAI/Custom LLM or Azure Providers will be available for LLM and Voice respectively.
This is due to the compliance requirements of HIPAA. Other providers may not meet these requirements. */
  hipaaEnabled?: boolean;
  /** This is the unique identifier for the org. */
  id: string;
  /**
   * This is the name of the org. This is just for your own reference.
   * @maxLength 40
   */
  name?: string;
  /** This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation. */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret. */
  serverUrlSecret?: string;
  /** This is the Stripe customer for the org. */
  stripeCustomerId?: string;
  /** This is the subscription's current period start. */
  stripeSubscriptionCurrentPeriodStart?: string;
  /** This is the subscription for the org. */
  stripeSubscriptionId?: string;
  /** This is the subscription's subscription item. */
  stripeSubscriptionItemId?: string;
  /** This is the subscription's status. */
  stripeSubscriptionStatus?: string;
  /** This is the ISO 8601 date-time string of when the org was last updated. */
  updatedAt: string;
}

export type UpdateVonageCredentialDTOProvider = typeof UpdateVonageCredentialDTOProvider[keyof typeof UpdateVonageCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateVonageCredentialDTOProvider = {
  vonage: 'vonage',
} as const;

export interface UpdateVonageCredentialDTO {
  apiKey: string;
  /** This is not returned in the API. */
  apiSecret: string;
  provider: UpdateVonageCredentialDTOProvider;
}

export type UpdateAnthropicCredentialDTOProvider = typeof UpdateAnthropicCredentialDTOProvider[keyof typeof UpdateAnthropicCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateAnthropicCredentialDTOProvider = {
  anthropic: 'anthropic',
} as const;

export interface UpdateAnthropicCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateAnthropicCredentialDTOProvider;
}

export type UpdateGroqCredentialDTOProvider = typeof UpdateGroqCredentialDTOProvider[keyof typeof UpdateGroqCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateGroqCredentialDTOProvider = {
  groq: 'groq',
} as const;

export interface UpdateGroqCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateGroqCredentialDTOProvider;
}

export type UpdateRunpodCredentialDTOProvider = typeof UpdateRunpodCredentialDTOProvider[keyof typeof UpdateRunpodCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateRunpodCredentialDTOProvider = {
  runpod: 'runpod',
} as const;

export interface UpdateRunpodCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateRunpodCredentialDTOProvider;
}

export type UpdateRimeAICredentialDTOProvider = typeof UpdateRimeAICredentialDTOProvider[keyof typeof UpdateRimeAICredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateRimeAICredentialDTOProvider = {
  'rime-ai': 'rime-ai',
} as const;

export interface UpdateRimeAICredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateRimeAICredentialDTOProvider;
}

export type UpdatePlayHTCredentialDTOProvider = typeof UpdatePlayHTCredentialDTOProvider[keyof typeof UpdatePlayHTCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdatePlayHTCredentialDTOProvider = {
  playht: 'playht',
} as const;

export interface UpdatePlayHTCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdatePlayHTCredentialDTOProvider;
  userId: string;
}

export type UpdateElevenLabsCredentialDTOProvider = typeof UpdateElevenLabsCredentialDTOProvider[keyof typeof UpdateElevenLabsCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateElevenLabsCredentialDTOProvider = {
  '11labs': '11labs',
} as const;

export interface UpdateElevenLabsCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateElevenLabsCredentialDTOProvider;
}

export type UpdateCustomLLMCredentialDTOProvider = typeof UpdateCustomLLMCredentialDTOProvider[keyof typeof UpdateCustomLLMCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateCustomLLMCredentialDTOProvider = {
  'custom-llm': 'custom-llm',
} as const;

export interface UpdateCustomLLMCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateCustomLLMCredentialDTOProvider;
}

export type UpdateDeepInfraCredentialDTOProvider = typeof UpdateDeepInfraCredentialDTOProvider[keyof typeof UpdateDeepInfraCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateDeepInfraCredentialDTOProvider = {
  deepinfra: 'deepinfra',
} as const;

export interface UpdateDeepInfraCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateDeepInfraCredentialDTOProvider;
}

export type UpdatePerplexityAICredentialDTOProvider = typeof UpdatePerplexityAICredentialDTOProvider[keyof typeof UpdatePerplexityAICredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdatePerplexityAICredentialDTOProvider = {
  'perplexity-ai': 'perplexity-ai',
} as const;

export interface UpdatePerplexityAICredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdatePerplexityAICredentialDTOProvider;
}

export type UpdateOpenRouterCredentialDTOProvider = typeof UpdateOpenRouterCredentialDTOProvider[keyof typeof UpdateOpenRouterCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateOpenRouterCredentialDTOProvider = {
  openrouter: 'openrouter',
} as const;

export interface UpdateOpenRouterCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateOpenRouterCredentialDTOProvider;
}

export type UpdateAnyscaleCredentialDTOProvider = typeof UpdateAnyscaleCredentialDTOProvider[keyof typeof UpdateAnyscaleCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateAnyscaleCredentialDTOProvider = {
  anyscale: 'anyscale',
} as const;

export interface UpdateAnyscaleCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateAnyscaleCredentialDTOProvider;
}

export type UpdateTogetherAICredentialDTOProvider = typeof UpdateTogetherAICredentialDTOProvider[keyof typeof UpdateTogetherAICredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateTogetherAICredentialDTOProvider = {
  'together-ai': 'together-ai',
} as const;

export interface UpdateTogetherAICredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateTogetherAICredentialDTOProvider;
}

export type UpdateOpenAICredentialDTOProvider = typeof UpdateOpenAICredentialDTOProvider[keyof typeof UpdateOpenAICredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateOpenAICredentialDTOProvider = {
  openai: 'openai',
} as const;

export interface UpdateOpenAICredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateOpenAICredentialDTOProvider;
}

export type UpdateDeepgramCredentialDTOProvider = typeof UpdateDeepgramCredentialDTOProvider[keyof typeof UpdateDeepgramCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateDeepgramCredentialDTOProvider = {
  deepgram: 'deepgram',
} as const;

export interface UpdateDeepgramCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: UpdateDeepgramCredentialDTOProvider;
}

export type UpdateTwilioCredentialDTOProvider = typeof UpdateTwilioCredentialDTOProvider[keyof typeof UpdateTwilioCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateTwilioCredentialDTOProvider = {
  twilio: 'twilio',
} as const;

export interface UpdateTwilioCredentialDTO {
  accountSid: string;
  /** This is not returned in the API. */
  authToken: string;
  provider: UpdateTwilioCredentialDTOProvider;
}

export type CreateVonageCredentialDTOProvider = typeof CreateVonageCredentialDTOProvider[keyof typeof CreateVonageCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateVonageCredentialDTOProvider = {
  vonage: 'vonage',
} as const;

export interface CreateVonageCredentialDTO {
  apiKey: string;
  /** This is not returned in the API. */
  apiSecret: string;
  provider: CreateVonageCredentialDTOProvider;
}

export type CreateAnthropicCredentialDTOProvider = typeof CreateAnthropicCredentialDTOProvider[keyof typeof CreateAnthropicCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateAnthropicCredentialDTOProvider = {
  anthropic: 'anthropic',
} as const;

export interface CreateAnthropicCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateAnthropicCredentialDTOProvider;
}

export type CreateGroqCredentialDTOProvider = typeof CreateGroqCredentialDTOProvider[keyof typeof CreateGroqCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateGroqCredentialDTOProvider = {
  groq: 'groq',
} as const;

export interface CreateGroqCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateGroqCredentialDTOProvider;
}

export type CreateRunpodCredentialDTOProvider = typeof CreateRunpodCredentialDTOProvider[keyof typeof CreateRunpodCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateRunpodCredentialDTOProvider = {
  runpod: 'runpod',
} as const;

export interface CreateRunpodCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateRunpodCredentialDTOProvider;
}

export type CreateRimeAICredentialDTOProvider = typeof CreateRimeAICredentialDTOProvider[keyof typeof CreateRimeAICredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateRimeAICredentialDTOProvider = {
  'rime-ai': 'rime-ai',
} as const;

export interface CreateRimeAICredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateRimeAICredentialDTOProvider;
}

export type CreatePlayHTCredentialDTOProvider = typeof CreatePlayHTCredentialDTOProvider[keyof typeof CreatePlayHTCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreatePlayHTCredentialDTOProvider = {
  playht: 'playht',
} as const;

export interface CreatePlayHTCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreatePlayHTCredentialDTOProvider;
  userId: string;
}

export type CreateElevenLabsCredentialDTOProvider = typeof CreateElevenLabsCredentialDTOProvider[keyof typeof CreateElevenLabsCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateElevenLabsCredentialDTOProvider = {
  '11labs': '11labs',
} as const;

export interface CreateElevenLabsCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateElevenLabsCredentialDTOProvider;
}

export type CreateCustomLLMCredentialDTOProvider = typeof CreateCustomLLMCredentialDTOProvider[keyof typeof CreateCustomLLMCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateCustomLLMCredentialDTOProvider = {
  'custom-llm': 'custom-llm',
} as const;

export interface CreateCustomLLMCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateCustomLLMCredentialDTOProvider;
}

export type CreateDeepInfraCredentialDTOProvider = typeof CreateDeepInfraCredentialDTOProvider[keyof typeof CreateDeepInfraCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateDeepInfraCredentialDTOProvider = {
  deepinfra: 'deepinfra',
} as const;

export interface CreateDeepInfraCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateDeepInfraCredentialDTOProvider;
}

export type CreatePerplexityAICredentialDTOProvider = typeof CreatePerplexityAICredentialDTOProvider[keyof typeof CreatePerplexityAICredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreatePerplexityAICredentialDTOProvider = {
  'perplexity-ai': 'perplexity-ai',
} as const;

export interface CreatePerplexityAICredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreatePerplexityAICredentialDTOProvider;
}

export type CreateOpenRouterCredentialDTOProvider = typeof CreateOpenRouterCredentialDTOProvider[keyof typeof CreateOpenRouterCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateOpenRouterCredentialDTOProvider = {
  openrouter: 'openrouter',
} as const;

export interface CreateOpenRouterCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateOpenRouterCredentialDTOProvider;
}

export type CreateAnyscaleCredentialDTOProvider = typeof CreateAnyscaleCredentialDTOProvider[keyof typeof CreateAnyscaleCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateAnyscaleCredentialDTOProvider = {
  anyscale: 'anyscale',
} as const;

export interface CreateAnyscaleCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateAnyscaleCredentialDTOProvider;
}

export type CreateTogetherAICredentialDTOProvider = typeof CreateTogetherAICredentialDTOProvider[keyof typeof CreateTogetherAICredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateTogetherAICredentialDTOProvider = {
  'together-ai': 'together-ai',
} as const;

export interface CreateTogetherAICredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateTogetherAICredentialDTOProvider;
}

export type CreateOpenAICredentialDTOProvider = typeof CreateOpenAICredentialDTOProvider[keyof typeof CreateOpenAICredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateOpenAICredentialDTOProvider = {
  openai: 'openai',
} as const;

export interface CreateOpenAICredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateOpenAICredentialDTOProvider;
}

export type CreateDeepgramCredentialDTOProvider = typeof CreateDeepgramCredentialDTOProvider[keyof typeof CreateDeepgramCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateDeepgramCredentialDTOProvider = {
  deepgram: 'deepgram',
} as const;

export interface CreateDeepgramCredentialDTO {
  /** This is not returned in the API. */
  apiKey: string;
  provider: CreateDeepgramCredentialDTOProvider;
}

export type CreateTwilioCredentialDTOProvider = typeof CreateTwilioCredentialDTOProvider[keyof typeof CreateTwilioCredentialDTOProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateTwilioCredentialDTOProvider = {
  twilio: 'twilio',
} as const;

export interface CreateTwilioCredentialDTO {
  accountSid: string;
  /** This is not returned in the API. */
  authToken: string;
  provider: CreateTwilioCredentialDTOProvider;
}

export type VonageCredentialProvider = typeof VonageCredentialProvider[keyof typeof VonageCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const VonageCredentialProvider = {
  vonage: 'vonage',
} as const;

export interface VonageCredential {
  apiKey: string;
  /** This is not returned in the API. */
  apiSecret: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: VonageCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
  /** This is the Vonage Application ID for the credential.

Only relevant for Vonage credentials. */
  vonageApplicationId: string;
  /** This is the Vonage Application Private Key for the credential.

Only relevant for Vonage credentials. */
  vonageApplicationPrivateKey: string;
}

export type AnthropicCredentialProvider = typeof AnthropicCredentialProvider[keyof typeof AnthropicCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AnthropicCredentialProvider = {
  anthropic: 'anthropic',
} as const;

export interface AnthropicCredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: AnthropicCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type GroqCredentialProvider = typeof GroqCredentialProvider[keyof typeof GroqCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const GroqCredentialProvider = {
  groq: 'groq',
} as const;

export interface GroqCredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: GroqCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type RunpodCredentialProvider = typeof RunpodCredentialProvider[keyof typeof RunpodCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const RunpodCredentialProvider = {
  runpod: 'runpod',
} as const;

export interface RunpodCredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: RunpodCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type CustomLLMCredentialProvider = typeof CustomLLMCredentialProvider[keyof typeof CustomLLMCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CustomLLMCredentialProvider = {
  'custom-llm': 'custom-llm',
} as const;

export interface CustomLLMCredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: CustomLLMCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type DeepInfraCredentialProvider = typeof DeepInfraCredentialProvider[keyof typeof DeepInfraCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const DeepInfraCredentialProvider = {
  deepinfra: 'deepinfra',
} as const;

export interface DeepInfraCredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: DeepInfraCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type PerplexityAICredentialProvider = typeof PerplexityAICredentialProvider[keyof typeof PerplexityAICredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const PerplexityAICredentialProvider = {
  'perplexity-ai': 'perplexity-ai',
} as const;

export interface PerplexityAICredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: PerplexityAICredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type OpenRouterCredentialProvider = typeof OpenRouterCredentialProvider[keyof typeof OpenRouterCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenRouterCredentialProvider = {
  openrouter: 'openrouter',
} as const;

export interface OpenRouterCredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: OpenRouterCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type AnyscaleCredentialProvider = typeof AnyscaleCredentialProvider[keyof typeof AnyscaleCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AnyscaleCredentialProvider = {
  anyscale: 'anyscale',
} as const;

export interface AnyscaleCredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: AnyscaleCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type TogetherAICredentialProvider = typeof TogetherAICredentialProvider[keyof typeof TogetherAICredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const TogetherAICredentialProvider = {
  'together-ai': 'together-ai',
} as const;

export interface TogetherAICredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: TogetherAICredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type OpenAICredentialProvider = typeof OpenAICredentialProvider[keyof typeof OpenAICredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenAICredentialProvider = {
  openai: 'openai',
} as const;

export interface OpenAICredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: OpenAICredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type RimeAICredentialProvider = typeof RimeAICredentialProvider[keyof typeof RimeAICredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const RimeAICredentialProvider = {
  'rime-ai': 'rime-ai',
} as const;

export interface RimeAICredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: RimeAICredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type PlayHTCredentialProvider = typeof PlayHTCredentialProvider[keyof typeof PlayHTCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const PlayHTCredentialProvider = {
  playht: 'playht',
} as const;

export interface PlayHTCredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: PlayHTCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
  userId: string;
}

export type ElevenLabsCredentialProvider = typeof ElevenLabsCredentialProvider[keyof typeof ElevenLabsCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ElevenLabsCredentialProvider = {
  '11labs': '11labs',
} as const;

export interface ElevenLabsCredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: ElevenLabsCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type DeepgramCredentialProvider = typeof DeepgramCredentialProvider[keyof typeof DeepgramCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const DeepgramCredentialProvider = {
  deepgram: 'deepgram',
} as const;

export interface DeepgramCredential {
  /** This is not returned in the API. */
  apiKey: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: DeepgramCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

export type TwilioCredentialProvider = typeof TwilioCredentialProvider[keyof typeof TwilioCredentialProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const TwilioCredentialProvider = {
  twilio: 'twilio',
} as const;

export interface TwilioCredential {
  accountSid: string;
  /** This is not returned in the API. */
  authToken: string;
  /** This is the ISO 8601 date-time string of when the credential was created. */
  createdAt: string;
  /** This is the unique identifier for the credential. */
  id: string;
  /** This is the unique identifier for the org that this credential belongs to. */
  orgId: string;
  provider: TwilioCredentialProvider;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
}

/**
 * This is the metadata associated with the call.
 */
export type CreateOutboundCallDTOMetadata = { [key: string]: any };

export interface CreateOutboundCallDTO {
  /** This is the assistant that will be used for the call. To use an existing assistant, use `assistantId` instead. */
  assistant?: CreateAssistantDTO;
  /**
   * This is the assistant that will be used for the call. To use a transient assistant, use `assistant` instead.
   * @nullable
   */
  assistantId?: string | null;
  /** Overrides for a single assistant's settings and template variables. */
  assistantOverride?: OverrideAssistantDTO;
  /** Individual overrides for multiple assistant settings and template variables.
If only one override is provided and multiple assistants are used, it will apply to all assistants.
Otherwise, the number of overrides must match the number of assistants. */
  assistantOverrides?: OverrideAssistantDTO[];
  /** This is the set of all assistants that can be used for the call.
The call can be transferred between these assistants.
The first assistant in the array will be the primary assistant that starts the call. */
  assistants?: CreateAssistantDTO[];
  /** This is the customer that will be called. To call an existing customer, use `customerId` instead.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  customer?: CreateCustomerDTO;
  /** This is the customer that will be called. To call a transient customer , use `customer` instead.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  customerId?: string;
  /**
   * This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
   * @minimum 10
   * @maximum 3600
   */
  maxDurationSeconds?: number;
  /** This is the metadata associated with the call. */
  metadata?: CreateOutboundCallDTOMetadata;
  /** This is the phone number that will be used for the call. To use an existing number, use `phoneNumberId` instead.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  phoneNumber?: ImportTwilioPhoneNumberDTO;
  /** This is the phone number that will be used for the call. To use a transient number, use `phoneNumber` instead.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  phoneNumberId?: string;
}

export interface PaginationMeta {
  currentPage: number;
  itemsPerPage: number;
  totalItems: number;
}

export interface CallPaginatedResponse {
  metadata: PaginationMeta;
  results: Call[];
}

/**
 * This is the type of call.
 */
export type CallType = typeof CallType[keyof typeof CallType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CallType = {
  inboundPhoneCall: 'inboundPhoneCall',
  outboundPhoneCall: 'outboundPhoneCall',
  webCall: 'webCall',
} as const;

/**
 * This is the status of the call.
 */
export type CallStatus = typeof CallStatus[keyof typeof CallStatus];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CallStatus = {
  queued: 'queued',
  ringing: 'ringing',
  'in-progress': 'in-progress',
  forwarding: 'forwarding',
  ended: 'ended',
} as const;

/**
 * This is the transport of the phone call.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type.
 */
export type CallPhoneCallTransport = typeof CallPhoneCallTransport[keyof typeof CallPhoneCallTransport];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CallPhoneCallTransport = {
  sip: 'sip',
  pstn: 'pstn',
} as const;

/**
 * This is the provider of the call.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type.
 */
export type CallPhoneCallProvider = typeof CallPhoneCallProvider[keyof typeof CallPhoneCallProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CallPhoneCallProvider = {
  twilio: 'twilio',
  vonage: 'vonage',
} as const;

/**
 * This is the metadata associated with the call.
 */
export type CallMetadata = { [key: string]: any };

export type CallMessagesItem = { [key: string]: any };

/**
 * This is the explanation for how the call ended.
 */
export type CallEndedReason = typeof CallEndedReason[keyof typeof CallEndedReason];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CallEndedReason = {
  'assistant-error': 'assistant-error',
  'assistant-not-found': 'assistant-not-found',
  'db-error': 'db-error',
  'no-server-available': 'no-server-available',
  'pipeline-error-extra-function-failed': 'pipeline-error-extra-function-failed',
  'pipeline-error-first-message-failed': 'pipeline-error-first-message-failed',
  'pipeline-error-function-filler-failed': 'pipeline-error-function-filler-failed',
  'pipeline-error-function-failed': 'pipeline-error-function-failed',
  'pipeline-error-openai-llm-failed': 'pipeline-error-openai-llm-failed',
  'pipeline-error-azure-openai-llm-failed': 'pipeline-error-azure-openai-llm-failed',
  'pipeline-error-together-ai-llm-failed': 'pipeline-error-together-ai-llm-failed',
  'pipeline-error-anyscale-llm-failed': 'pipeline-error-anyscale-llm-failed',
  'pipeline-error-openrouter-llm-failed': 'pipeline-error-openrouter-llm-failed',
  'pipeline-error-perplexity-ai-llm-failed': 'pipeline-error-perplexity-ai-llm-failed',
  'pipeline-error-deepinfra-llm-failed': 'pipeline-error-deepinfra-llm-failed',
  'pipeline-error-runpod-llm-failed': 'pipeline-error-runpod-llm-failed',
  'pipeline-error-groq-llm-failed': 'pipeline-error-groq-llm-failed',
  'pipeline-error-anthropic-llm-failed': 'pipeline-error-anthropic-llm-failed',
  'pipeline-error-openai-voice-failed': 'pipeline-error-openai-voice-failed',
  'pipeline-error-deepgram-transcriber-failed': 'pipeline-error-deepgram-transcriber-failed',
  'pipeline-error-deepgram-voice-failed': 'pipeline-error-deepgram-voice-failed',
  'pipeline-error-eleven-labs-voice-failed': 'pipeline-error-eleven-labs-voice-failed',
  'pipeline-error-playht-voice-failed': 'pipeline-error-playht-voice-failed',
  'pipeline-error-lmnt-voice-failed': 'pipeline-error-lmnt-voice-failed',
  'pipeline-error-azure-voice-failed': 'pipeline-error-azure-voice-failed',
  'pipeline-error-rime-ai-voice-failed': 'pipeline-error-rime-ai-voice-failed',
  'pipeline-error-neets-voice-failed': 'pipeline-error-neets-voice-failed',
  'pipeline-no-available-llm-model': 'pipeline-no-available-llm-model',
  'server-shutdown': 'server-shutdown',
  'twilio-failed-to-connect-call': 'twilio-failed-to-connect-call',
  'unknown-error': 'unknown-error',
  'vonage-disconnected': 'vonage-disconnected',
  'vonage-failed-to-connect-call': 'vonage-failed-to-connect-call',
  'phone-call-provider-bypass-enabled-but-no-call-received': 'phone-call-provider-bypass-enabled-but-no-call-received',
  'vapi-error-phone-call-worker-setup-socket-error': 'vapi-error-phone-call-worker-setup-socket-error',
  'vapi-error-phone-call-worker-worker-setup-socket-timeout': 'vapi-error-phone-call-worker-worker-setup-socket-timeout',
  'vapi-error-phone-call-worker-could-not-find-call': 'vapi-error-phone-call-worker-could-not-find-call',
  'vapi-error-phone-call-worker-call-never-connected': 'vapi-error-phone-call-worker-call-never-connected',
  'vapi-error-web-call-worker-setup-failed': 'vapi-error-web-call-worker-setup-failed',
  'assistant-not-invalid': 'assistant-not-invalid',
  'assistant-not-provided': 'assistant-not-provided',
  'assistant-request-returned-error': 'assistant-request-returned-error',
  'assistant-request-returned-invalid-assistant': 'assistant-request-returned-invalid-assistant',
  'assistant-request-returned-no-assistant': 'assistant-request-returned-no-assistant',
  'assistant-request-returned-forwarding-phone-number': 'assistant-request-returned-forwarding-phone-number',
  'assistant-ended-call': 'assistant-ended-call',
  'assistant-said-end-call-phrase': 'assistant-said-end-call-phrase',
  'assistant-forwarded-call': 'assistant-forwarded-call',
  'assistant-join-timed-out': 'assistant-join-timed-out',
  'customer-busy': 'customer-busy',
  'customer-ended-call': 'customer-ended-call',
  'customer-did-not-answer': 'customer-did-not-answer',
  'customer-did-not-give-microphone-permission': 'customer-did-not-give-microphone-permission',
  'exceeded-max-duration': 'exceeded-max-duration',
  'manually-canceled': 'manually-canceled',
  'phone-call-provider-closed-websocket': 'phone-call-provider-closed-websocket',
  'pipeline-error-custom-llm-llm-failed': 'pipeline-error-custom-llm-llm-failed',
  'pipeline-error-eleven-labs-voice-not-found': 'pipeline-error-eleven-labs-voice-not-found',
  'pipeline-error-eleven-labs-quota-exceeded': 'pipeline-error-eleven-labs-quota-exceeded',
  'pipeline-error-eleven-labs-blocked-free-plan': 'pipeline-error-eleven-labs-blocked-free-plan',
  'pipeline-error-eleven-labs-blocked-concurrent-requests': 'pipeline-error-eleven-labs-blocked-concurrent-requests',
  'pipeline-error-eleven-labs-unauthorized-access': 'pipeline-error-eleven-labs-unauthorized-access',
  'pipeline-error-eleven-labs-system-busy-and-requested-upgrade': 'pipeline-error-eleven-labs-system-busy-and-requested-upgrade',
  'pipeline-error-eleven-labs-voice-not-fine-tuned': 'pipeline-error-eleven-labs-voice-not-fine-tuned',
  'pipeline-error-playht-request-timed-out': 'pipeline-error-playht-request-timed-out',
  'pipeline-error-playht-invalid-voice': 'pipeline-error-playht-invalid-voice',
  'pipeline-error-playht-unexpected-error': 'pipeline-error-playht-unexpected-error',
  'pipeline-error-playht-out-of-credits': 'pipeline-error-playht-out-of-credits',
  'pipeline-error-playht-rate-limit-exceeded': 'pipeline-error-playht-rate-limit-exceeded',
  'pipeline-error-playht-502-gateway-error': 'pipeline-error-playht-502-gateway-error',
  'pipeline-error-playht-504-gateway-error': 'pipeline-error-playht-504-gateway-error',
  'silence-timed-out': 'silence-timed-out',
  voicemail: 'voicemail',
  'vonage-rejected': 'vonage-rejected',
} as const;

export interface ImportTwilioPhoneNumberDTO {
  /**
   * This is the assistant that will be used for incoming calls to this phone number.

If this is not set, then the phone number will not handle incoming calls.
   * @nullable
   */
  assistantId?: string | null;
  /**
   * This is the name of the phone number. This is just for your own reference.
   * @maxLength 40
   */
  name?: string;
  /** This is the server URL that will be used to handle this phone number.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.

This overrides the serverUrl set on the org. Order of precedence: function.serverUrl > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl. */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.

Same precendence logic as serverUrl. */
  serverUrlSecret?: string;
  /** This is your Twilio Account SID that will be used to handle this phone number. */
  twilioAccountSid: string;
  /** This is the Twilio Auth Token that will be used to handle this phone number. */
  twilioAuthToken: string;
  /** These are the digits of the phone number you own on your Twilio. */
  twilioPhoneNumber: string;
}

export interface CreateCustomerDTO {
  /**
   * This is the extension that will be dialed after the call is answered.
   * @maxLength 30
   */
  extension?: string;
  /**
   * This is the name of the customer. This is just for your own reference.
   * @maxLength 40
   */
  name?: string;
  /** This is the number of the customer. */
  number?: string;
}

export interface Call {
  /** This is the assistant that will be used for the call. To use an existing assistant, use `assistantId` instead. */
  assistant?: CreateAssistantDTO;
  /**
   * This is the assistant that will be used for the call. To use a transient assistant, use `assistant` instead.
   * @nullable
   */
  assistantId?: string | null;
  /** Overrides for a single assistant's settings and template variables. */
  assistantOverride?: OverrideAssistantDTO;
  /** Individual overrides for multiple assistant settings and template variables.
If only one override is provided and multiple assistants are used, it will apply to all assistants.
Otherwise, the number of overrides must match the number of assistants. */
  assistantOverrides?: OverrideAssistantDTO[];
  /** This is the set of all assistants that can be used for the call.
The call can be transferred between these assistants.
The first assistant in the array will be the primary assistant that starts the call. */
  assistants?: CreateAssistantDTO[];
  /** This is the cost of the call in USD. */
  cost?: number;
  /** This is the cost of the call in USD. */
  costBreakdown?: CostBreakdown;
  /** This is the ISO 8601 date-time string of when the call was created. */
  createdAt: string;
  /** This is the customer that will be called. To call an existing customer, use `customerId` instead.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  customer?: CreateCustomerDTO;
  /** This is the customer that will be called. To call a transient customer , use `customer` instead.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  customerId?: string;
  /** This is the ISO 8601 date-time string of when the call was ended. */
  endedAt?: string;
  /** This is the explanation for how the call ended. */
  endedReason?: CallEndedReason;
  /** This is the phone number that the call was forwarded to. */
  forwardedPhoneNumber?: string;
  /** This is the unique identifier for the call. */
  id: string;
  /**
   * This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
   * @minimum 10
   * @maximum 3600
   */
  maxDurationSeconds?: number;
  /** These are the messages that were spoken during the call. */
  messages?: CallMessagesItem[];
  /** This is the metadata associated with the call. */
  metadata?: CallMetadata;
  /** This is the unique identifier for the org that this call belongs to. */
  orgId: string;
  /** This is the provider of the call.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  phoneCallProvider?: CallPhoneCallProvider;
  /** If enabled, prevents Vapi from initiating calls directly. Defaults to disabled.
Suitable for external call handling, such as with Twilio Studio Flow, with integration details provided in `phoneCallProviderDetails`.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` types. */
  phoneCallProviderBypassEnabled?: boolean;
  /** This is the phone call provider details to bridge the assistant into the external call. Only filled if `phoneCallProviderBypassEnabled` is true.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` types. */
  phoneCallProviderDetails?: PhoneCallTwilioDetails;
  /** The ID of the call as provided by the phone number service. callSid in Twilio. conversationUuid in Vonage.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  phoneCallProviderId?: string;
  /** This is the transport of the phone call.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  phoneCallTransport?: CallPhoneCallTransport;
  /** This is the phone number that will be used for the call. To use an existing number, use `phoneNumberId` instead.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  phoneNumber?: ImportTwilioPhoneNumberDTO;
  /** This is the phone number that will be used for the call. To use a transient number, use `phoneNumber` instead.

Only relevant for `outboundPhoneCall` and `inboundPhoneCall` type. */
  phoneNumberId?: string;
  /** This is the URL of the recording of the call. */
  recordingUrl?: string;
  /** This is the ISO 8601 date-time string of when the call was started. */
  startedAt?: string;
  /** This is the status of the call. */
  status?: CallStatus;
  /** This is the URL of the recording of the call in two channels. */
  stereoRecordingUrl?: string;
  /** This is the summary of the call. */
  summary?: string;
  /** This is the transcript of the call. */
  transcript?: string;
  /** This is the type of call. */
  type?: CallType;
  /** This is the ISO 8601 date-time string of when the call was last updated. */
  updatedAt: string;
  /** This is the SIP URI of the call that the assistant will join.

Only relevant for `webCall` type. */
  webCallSipUri?: string;
  webCallUrl?: string;
}

export type OverrideAssistantDTOVoicemailDetectionTypesItem = typeof OverrideAssistantDTOVoicemailDetectionTypesItem[keyof typeof OverrideAssistantDTOVoicemailDetectionTypesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OverrideAssistantDTOVoicemailDetectionTypesItem = {
  machine_start: 'machine_start',
  human: 'human',
  fax: 'fax',
  unknown: 'unknown',
  machine_end_beep: 'machine_end_beep',
  machine_end_silence: 'machine_end_silence',
  machine_end_other: 'machine_end_other',
} as const;

/**
 * These are the options for the assistant's voice.
 */
export type OverrideAssistantDTOVoice = AzureVoice | ElevenLabsVoice | PlayHTVoice | RimeAIVoice | DeepgramVoice | OpenAIVoice | LMNTVoice | NeetsVoice;

/**
 * These are template variables that will be replaced in the assistant messages and prompts.
 */
export type OverrideAssistantDTOVariableValues = { [key: string]: any };

/**
 * These are the options for the assistant's transcriber.
 */
export type OverrideAssistantDTOTranscriber = DeepgramTranscriber | TalkscriberTranscriber;

export type OverrideAssistantDTOServerMessagesItem = typeof OverrideAssistantDTOServerMessagesItem[keyof typeof OverrideAssistantDTOServerMessagesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OverrideAssistantDTOServerMessagesItem = {
  'status-update': 'status-update',
  transcript: 'transcript',
  hang: 'hang',
  'function-call': 'function-call',
  'tool-calls': 'tool-calls',
  'end-of-call-report': 'end-of-call-report',
  'conversation-update': 'conversation-update',
  'phone-call-control': 'phone-call-control',
  'model-output': 'model-output',
  'voice-input': 'voice-input',
} as const;

/**
 * These are the options for the assistant's LLM.
 */
export type OverrideAssistantDTOModel = OpenAIModel | TogetherAIModel | AnyscaleModel | OpenRouterModel | PerplexityAIModel | DeepInfraModel | GroqModel | AnthropicModel | CustomLLMModel;

/**
 * This is the metadata associated with the call.
 */
export type OverrideAssistantDTOMetadata = { [key: string]: any };

/**
 * This is the mode for the first message. Default is 'assistant-speaks-first'.

Specify 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
 */
export type OverrideAssistantDTOFirstMessageMode = typeof OverrideAssistantDTOFirstMessageMode[keyof typeof OverrideAssistantDTOFirstMessageMode];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OverrideAssistantDTOFirstMessageMode = {
  'assistant-speaks-first': 'assistant-speaks-first',
  'assistant-waits-for-user': 'assistant-waits-for-user',
} as const;

export type OverrideAssistantDTOClientMessagesItem = typeof OverrideAssistantDTOClientMessagesItem[keyof typeof OverrideAssistantDTOClientMessagesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OverrideAssistantDTOClientMessagesItem = {
  'status-update': 'status-update',
  'speech-update': 'speech-update',
  transcript: 'transcript',
  hang: 'hang',
  'function-call': 'function-call',
  'tool-calls': 'tool-calls',
  metadata: 'metadata',
  'conversation-update': 'conversation-update',
  'model-output': 'model-output',
  'voice-input': 'voice-input',
} as const;

/**
 * This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
 */
export type OverrideAssistantDTOBackgroundSound = typeof OverrideAssistantDTOBackgroundSound[keyof typeof OverrideAssistantDTOBackgroundSound];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OverrideAssistantDTOBackgroundSound = {
  off: 'off',
  office: 'office',
} as const;

export interface OverrideAssistantDTO {
  /** backchanneling is the bot say while listening like 'mhmm', 'ahem' etc. this make the conversation sounds natural. Default True */
  backchannelingEnabled?: boolean;
  /** This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'. */
  backgroundSound?: OverrideAssistantDTOBackgroundSound;
  /** These are the messages that will be sent to the Client SDKs. Default is ['transcript', 'hang', 'tool-calls', 'speech-update', 'metadata', 'conversation-update'] */
  clientMessages?: OverrideAssistantDTOClientMessagesItem[];
  /** This sets whether the assistant can dial digits on the keypad. Defaults to false. */
  dialKeypadFunctionEnabled?: boolean;
  /** This sets whether the assistant will be able to hang up the call. Defaults to false. */
  endCallFunctionEnabled?: boolean;
  /**
   * This is the message that the assistant will say if it ends the call.

If unspecified, it will hang up without saying anything.
   * @maxLength 400
   */
  endCallMessage?: string;
  /** This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive. */
  endCallPhrases?: string[];
  /**
   * This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).

If unspecified, assistant will wait for user to speak and use the model to respond once they speak.
   * @maxLength 1000
   */
  firstMessage?: string;
  /** This is the mode for the first message. Default is 'assistant-speaks-first'.

Specify 'assistant-waits-for-user' to have the assistant wait for the user to speak first. */
  firstMessageMode?: OverrideAssistantDTOFirstMessageMode;
  forwardingPhoneNumber?: string;
  /** When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false. */
  hipaaEnabled?: boolean;
  /**
   * The minimum number of seconds to wait after punctuation before sending a request to the LLM. Defaults to 0.1.
   * @minimum 0
   * @maximum 3
   */
  llmRequestDelaySeconds?: number;
  /**
   * This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
   * @minimum 10
   * @maximum 3600
   */
  maxDurationSeconds?: number;
  /** This is the metadata associated with the call. */
  metadata?: OverrideAssistantDTOMetadata;
  /** These are the options for the assistant's LLM. */
  model?: OverrideAssistantDTOModel;
  /**
   * This is the name of the assistant.
This is only required to transfer calls between assistants.
   * @maxLength 100
   */
  name?: string;
  /**
   * The number of words to wait for before interrupting the assistant. Words like "stop", "actually", "no", etc. will always interrupt immediately regardless of this value. Words like "okay", "yeah", "right" will never interrupt. Defaults to 1.
   * @minimum 1
   * @maximum 10
   */
  numWordsToInterruptAssistant?: number;
  /** This sets whether the assistant's calls are recorded. Defaults to true. */
  recordingEnabled?: boolean;
  /**
   * The minimum number of seconds after user speech to wait before the assistant starts speaking. Defaults to 0.4.
   * @minimum 0
   * @maximum 5
   */
  responseDelaySeconds?: number;
  /** These are the messages that will be sent to your Server URL. Default is ['end-of-call-report', 'status-update', 'hang', 'tool-calls'] */
  serverMessages?: OverrideAssistantDTOServerMessagesItem[];
  /** This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.

This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: function.serverUrl > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.

Same precedence logic as serverUrl. */
  serverUrlSecret?: string;
  /**
   * How many seconds of silence to wait before ending the call. Defaults to 30.
   * @minimum 10
   * @maximum 600
   */
  silenceTimeoutSeconds?: number;
  /**
   * This is the prompt that's used to summarize the call at the end.

Default is 'You are an expert note-taker. You will be given a transcript of a conversation. Please summarize the conversation in 4-5 sentences if applicable.'

Set to '' or 'off' to disable post-call summarization.
   * @maxLength 2000
   */
  summaryPrompt?: string;
  /** These are the options for the assistant's transcriber. */
  transcriber?: OverrideAssistantDTOTranscriber;
  /** These are template variables that will be replaced in the assistant messages and prompts. */
  variableValues?: OverrideAssistantDTOVariableValues;
  /** These are the options for the assistant's voice. */
  voice?: OverrideAssistantDTOVoice;
  /** This sets whether the assistant should detect voicemail. Defaults to true. */
  voicemailDetectionEnabled?: boolean;
  /** These are the AMD messages from Twilio that are considered as voicemail. Default is ['machine_end_beep', 'machine_end_silence', 'machine_end_other']. */
  voicemailDetectionTypes?: OverrideAssistantDTOVoicemailDetectionTypesItem[];
  /**
   * This is the message that the assistant will say if the call is forwarded to voicemail.

If unspecified, it will hang up.
   * @maxLength 1000
   */
  voicemailMessage?: string;
}

export type PhoneCallTwilioDetailsStatusCallbackEvent = typeof PhoneCallTwilioDetailsStatusCallbackEvent[keyof typeof PhoneCallTwilioDetailsStatusCallbackEvent];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const PhoneCallTwilioDetailsStatusCallbackEvent = {
  initiated: 'initiated',
  ringing: 'ringing',
  answered: 'answered',
  completed: 'completed',
} as const;

export type PhoneCallTwilioDetailsMachineDetection = typeof PhoneCallTwilioDetailsMachineDetection[keyof typeof PhoneCallTwilioDetailsMachineDetection];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const PhoneCallTwilioDetailsMachineDetection = {
  Enable: 'Enable',
  DetectMessageEnd: 'DetectMessageEnd',
} as const;

export interface PhoneCallTwilioDetails {
  asyncAmd?: string;
  asyncAmdStatusCallback?: string;
  from: string;
  machineDetection?: PhoneCallTwilioDetailsMachineDetection;
  record?: boolean;
  statusCallback?: string;
  statusCallbackEvent?: PhoneCallTwilioDetailsStatusCallbackEvent;
  to: string;
  twiml?: string;
}

export interface CostBreakdown {
  /** This is the cost of the language model. */
  llm?: number;
  /** This is the LLM completion tokens used for the call. */
  llmCompletionTokens?: number;
  /** This is the LLM prompt tokens used for the call. */
  llmPromptTokens?: number;
  /** This is the cost of the speech-to-text service. */
  stt?: number;
  /** This is the total cost of the call. */
  total?: number;
  /** This is the cost of the transport provider, like Twilio or Vonage. */
  transport?: number;
  /** This is the cost of the text-to-speech service. */
  tts?: number;
  /** This is the TTS characters used for the call. */
  ttsCharacters?: number;
  /** This is the cost of Vapi. */
  vapi?: number;
}

export type UpdateAssistantDTOVoicemailDetectionTypesItem = typeof UpdateAssistantDTOVoicemailDetectionTypesItem[keyof typeof UpdateAssistantDTOVoicemailDetectionTypesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateAssistantDTOVoicemailDetectionTypesItem = {
  machine_start: 'machine_start',
  human: 'human',
  fax: 'fax',
  unknown: 'unknown',
  machine_end_beep: 'machine_end_beep',
  machine_end_silence: 'machine_end_silence',
  machine_end_other: 'machine_end_other',
} as const;

/**
 * These are the options for the assistant's voice.
 */
export type UpdateAssistantDTOVoice = AzureVoice | ElevenLabsVoice | PlayHTVoice | RimeAIVoice | DeepgramVoice | OpenAIVoice | LMNTVoice | NeetsVoice;

/**
 * These are the options for the assistant's transcriber.
 */
export type UpdateAssistantDTOTranscriber = DeepgramTranscriber | TalkscriberTranscriber;

export type UpdateAssistantDTOServerMessagesItem = typeof UpdateAssistantDTOServerMessagesItem[keyof typeof UpdateAssistantDTOServerMessagesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateAssistantDTOServerMessagesItem = {
  'status-update': 'status-update',
  transcript: 'transcript',
  hang: 'hang',
  'function-call': 'function-call',
  'tool-calls': 'tool-calls',
  'end-of-call-report': 'end-of-call-report',
  'conversation-update': 'conversation-update',
  'phone-call-control': 'phone-call-control',
  'model-output': 'model-output',
  'voice-input': 'voice-input',
} as const;

/**
 * These are the options for the assistant's LLM.
 */
export type UpdateAssistantDTOModel = OpenAIModel | TogetherAIModel | AnyscaleModel | OpenRouterModel | PerplexityAIModel | DeepInfraModel | GroqModel | AnthropicModel | CustomLLMModel;

/**
 * This is the metadata associated with the call.
 */
export type UpdateAssistantDTOMetadata = { [key: string]: any };

/**
 * This is the mode for the first message. Default is 'assistant-speaks-first'.

Specify 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
 */
export type UpdateAssistantDTOFirstMessageMode = typeof UpdateAssistantDTOFirstMessageMode[keyof typeof UpdateAssistantDTOFirstMessageMode];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateAssistantDTOFirstMessageMode = {
  'assistant-speaks-first': 'assistant-speaks-first',
  'assistant-waits-for-user': 'assistant-waits-for-user',
} as const;

export type UpdateAssistantDTOClientMessagesItem = typeof UpdateAssistantDTOClientMessagesItem[keyof typeof UpdateAssistantDTOClientMessagesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateAssistantDTOClientMessagesItem = {
  'status-update': 'status-update',
  'speech-update': 'speech-update',
  transcript: 'transcript',
  hang: 'hang',
  'function-call': 'function-call',
  'tool-calls': 'tool-calls',
  metadata: 'metadata',
  'conversation-update': 'conversation-update',
  'model-output': 'model-output',
  'voice-input': 'voice-input',
} as const;

/**
 * This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
 */
export type UpdateAssistantDTOBackgroundSound = typeof UpdateAssistantDTOBackgroundSound[keyof typeof UpdateAssistantDTOBackgroundSound];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const UpdateAssistantDTOBackgroundSound = {
  off: 'off',
  office: 'office',
} as const;

export interface UpdateAssistantDTO {
  /** backchanneling is the bot say while listening like 'mhmm', 'ahem' etc. this make the conversation sounds natural. Default True */
  backchannelingEnabled?: boolean;
  /** This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'. */
  backgroundSound?: UpdateAssistantDTOBackgroundSound;
  /** These are the messages that will be sent to the Client SDKs. Default is ['transcript', 'hang', 'tool-calls', 'speech-update', 'metadata', 'conversation-update'] */
  clientMessages?: UpdateAssistantDTOClientMessagesItem[];
  /** This sets whether the assistant can dial digits on the keypad. Defaults to false. */
  dialKeypadFunctionEnabled?: boolean;
  /** This sets whether the assistant will be able to hang up the call. Defaults to false. */
  endCallFunctionEnabled?: boolean;
  /**
   * This is the message that the assistant will say if it ends the call.

If unspecified, it will hang up without saying anything.
   * @maxLength 400
   */
  endCallMessage?: string;
  /** This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive. */
  endCallPhrases?: string[];
  /**
   * This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).

If unspecified, assistant will wait for user to speak and use the model to respond once they speak.
   * @maxLength 1000
   */
  firstMessage?: string;
  /** This is the mode for the first message. Default is 'assistant-speaks-first'.

Specify 'assistant-waits-for-user' to have the assistant wait for the user to speak first. */
  firstMessageMode?: UpdateAssistantDTOFirstMessageMode;
  forwardingPhoneNumber?: string;
  /** When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false. */
  hipaaEnabled?: boolean;
  /**
   * The minimum number of seconds to wait after punctuation before sending a request to the LLM. Defaults to 0.1.
   * @minimum 0
   * @maximum 3
   */
  llmRequestDelaySeconds?: number;
  /**
   * This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
   * @minimum 10
   * @maximum 3600
   */
  maxDurationSeconds?: number;
  /** This is the metadata associated with the call. */
  metadata?: UpdateAssistantDTOMetadata;
  /** These are the options for the assistant's LLM. */
  model?: UpdateAssistantDTOModel;
  /**
   * This is the name of the assistant.
This is only required to transfer calls between assistants.
   * @maxLength 100
   */
  name?: string;
  /**
   * The number of words to wait for before interrupting the assistant. Words like "stop", "actually", "no", etc. will always interrupt immediately regardless of this value. Words like "okay", "yeah", "right" will never interrupt. Defaults to 1.
   * @minimum 1
   * @maximum 10
   */
  numWordsToInterruptAssistant?: number;
  /** This sets whether the assistant's calls are recorded. Defaults to true. */
  recordingEnabled?: boolean;
  /**
   * The minimum number of seconds after user speech to wait before the assistant starts speaking. Defaults to 0.4.
   * @minimum 0
   * @maximum 5
   */
  responseDelaySeconds?: number;
  /** These are the messages that will be sent to your Server URL. Default is ['end-of-call-report', 'status-update', 'hang', 'tool-calls'] */
  serverMessages?: UpdateAssistantDTOServerMessagesItem[];
  /** This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.

This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: function.serverUrl > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.

Same precedence logic as serverUrl. */
  serverUrlSecret?: string;
  /**
   * How many seconds of silence to wait before ending the call. Defaults to 30.
   * @minimum 10
   * @maximum 600
   */
  silenceTimeoutSeconds?: number;
  /**
   * This is the prompt that's used to summarize the call at the end.

Default is 'You are an expert note-taker. You will be given a transcript of a conversation. Please summarize the conversation in 4-5 sentences if applicable.'

Set to '' or 'off' to disable post-call summarization.
   * @maxLength 2000
   */
  summaryPrompt?: string;
  /** These are the options for the assistant's transcriber. */
  transcriber?: UpdateAssistantDTOTranscriber;
  /** These are the options for the assistant's voice. */
  voice?: UpdateAssistantDTOVoice;
  /** This sets whether the assistant should detect voicemail. Defaults to true. */
  voicemailDetectionEnabled?: boolean;
  /** These are the AMD messages from Twilio that are considered as voicemail. Default is ['machine_end_beep', 'machine_end_silence', 'machine_end_other']. */
  voicemailDetectionTypes?: UpdateAssistantDTOVoicemailDetectionTypesItem[];
  /**
   * This is the message that the assistant will say if the call is forwarded to voicemail.

If unspecified, it will hang up.
   * @maxLength 1000
   */
  voicemailMessage?: string;
}

export type AssistantVoicemailDetectionTypesItem = typeof AssistantVoicemailDetectionTypesItem[keyof typeof AssistantVoicemailDetectionTypesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AssistantVoicemailDetectionTypesItem = {
  machine_start: 'machine_start',
  human: 'human',
  fax: 'fax',
  unknown: 'unknown',
  machine_end_beep: 'machine_end_beep',
  machine_end_silence: 'machine_end_silence',
  machine_end_other: 'machine_end_other',
} as const;

/**
 * These are the options for the assistant's voice.
 */
export type AssistantVoice = AzureVoice | ElevenLabsVoice | PlayHTVoice | RimeAIVoice | DeepgramVoice | OpenAIVoice | LMNTVoice | NeetsVoice;

/**
 * These are the options for the assistant's transcriber.
 */
export type AssistantTranscriber = DeepgramTranscriber | TalkscriberTranscriber;

export type AssistantServerMessagesItem = typeof AssistantServerMessagesItem[keyof typeof AssistantServerMessagesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AssistantServerMessagesItem = {
  'status-update': 'status-update',
  transcript: 'transcript',
  hang: 'hang',
  'function-call': 'function-call',
  'tool-calls': 'tool-calls',
  'end-of-call-report': 'end-of-call-report',
  'conversation-update': 'conversation-update',
  'phone-call-control': 'phone-call-control',
  'model-output': 'model-output',
  'voice-input': 'voice-input',
} as const;

/**
 * These are the options for the assistant's LLM.
 */
export type AssistantModel = OpenAIModel | TogetherAIModel | AnyscaleModel | OpenRouterModel | PerplexityAIModel | DeepInfraModel | GroqModel | AnthropicModel | CustomLLMModel;

/**
 * This is the metadata associated with the call.
 */
export type AssistantMetadata = { [key: string]: any };

/**
 * This is the mode for the first message. Default is 'assistant-speaks-first'.

Specify 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
 */
export type AssistantFirstMessageMode = typeof AssistantFirstMessageMode[keyof typeof AssistantFirstMessageMode];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AssistantFirstMessageMode = {
  'assistant-speaks-first': 'assistant-speaks-first',
  'assistant-waits-for-user': 'assistant-waits-for-user',
} as const;

export type AssistantClientMessagesItem = typeof AssistantClientMessagesItem[keyof typeof AssistantClientMessagesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AssistantClientMessagesItem = {
  'status-update': 'status-update',
  'speech-update': 'speech-update',
  transcript: 'transcript',
  hang: 'hang',
  'function-call': 'function-call',
  'tool-calls': 'tool-calls',
  metadata: 'metadata',
  'conversation-update': 'conversation-update',
  'model-output': 'model-output',
  'voice-input': 'voice-input',
} as const;

/**
 * This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
 */
export type AssistantBackgroundSound = typeof AssistantBackgroundSound[keyof typeof AssistantBackgroundSound];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AssistantBackgroundSound = {
  off: 'off',
  office: 'office',
} as const;

export interface Assistant {
  /** backchanneling is the bot say while listening like 'mhmm', 'ahem' etc. this make the conversation sounds natural. Default True */
  backchannelingEnabled?: boolean;
  /** This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'. */
  backgroundSound?: AssistantBackgroundSound;
  /** These are the messages that will be sent to the Client SDKs. Default is ['transcript', 'hang', 'tool-calls', 'speech-update', 'metadata', 'conversation-update'] */
  clientMessages?: AssistantClientMessagesItem[];
  /** This is the ISO 8601 date-time string of when the assistant was created. */
  createdAt: string;
  /** This sets whether the assistant can dial digits on the keypad. Defaults to false. */
  dialKeypadFunctionEnabled?: boolean;
  /** This sets whether the assistant will be able to hang up the call. Defaults to false. */
  endCallFunctionEnabled?: boolean;
  /**
   * This is the message that the assistant will say if it ends the call.

If unspecified, it will hang up without saying anything.
   * @maxLength 400
   */
  endCallMessage?: string;
  /** This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive. */
  endCallPhrases?: string[];
  /**
   * This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).

If unspecified, assistant will wait for user to speak and use the model to respond once they speak.
   * @maxLength 1000
   */
  firstMessage?: string;
  /** This is the mode for the first message. Default is 'assistant-speaks-first'.

Specify 'assistant-waits-for-user' to have the assistant wait for the user to speak first. */
  firstMessageMode?: AssistantFirstMessageMode;
  forwardingPhoneNumber?: string;
  /** When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false. */
  hipaaEnabled?: boolean;
  /** This is the unique identifier for the assistant. */
  id: string;
  /**
   * The minimum number of seconds to wait after punctuation before sending a request to the LLM. Defaults to 0.1.
   * @minimum 0
   * @maximum 3
   */
  llmRequestDelaySeconds?: number;
  /**
   * This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
   * @minimum 10
   * @maximum 3600
   */
  maxDurationSeconds?: number;
  /** This is the metadata associated with the call. */
  metadata?: AssistantMetadata;
  /** These are the options for the assistant's LLM. */
  model?: AssistantModel;
  /**
   * This is the name of the assistant.
This is only required to transfer calls between assistants.
   * @maxLength 100
   */
  name?: string;
  /**
   * The number of words to wait for before interrupting the assistant. Words like "stop", "actually", "no", etc. will always interrupt immediately regardless of this value. Words like "okay", "yeah", "right" will never interrupt. Defaults to 1.
   * @minimum 1
   * @maximum 10
   */
  numWordsToInterruptAssistant?: number;
  /** This is the unique identifier for the org that this assistant belongs to. */
  orgId: string;
  /** This sets whether the assistant's calls are recorded. Defaults to true. */
  recordingEnabled?: boolean;
  /**
   * The minimum number of seconds after user speech to wait before the assistant starts speaking. Defaults to 0.4.
   * @minimum 0
   * @maximum 5
   */
  responseDelaySeconds?: number;
  /** These are the messages that will be sent to your Server URL. Default is ['end-of-call-report', 'status-update', 'hang', 'tool-calls'] */
  serverMessages?: AssistantServerMessagesItem[];
  /** This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.

This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: function.serverUrl > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.

Same precedence logic as serverUrl. */
  serverUrlSecret?: string;
  /**
   * How many seconds of silence to wait before ending the call. Defaults to 30.
   * @minimum 10
   * @maximum 600
   */
  silenceTimeoutSeconds?: number;
  /**
   * This is the prompt that's used to summarize the call at the end.

Default is 'You are an expert note-taker. You will be given a transcript of a conversation. Please summarize the conversation in 4-5 sentences if applicable.'

Set to '' or 'off' to disable post-call summarization.
   * @maxLength 2000
   */
  summaryPrompt?: string;
  /** These are the options for the assistant's transcriber. */
  transcriber?: AssistantTranscriber;
  /** This is the ISO 8601 date-time string of when the assistant was last updated. */
  updatedAt: string;
  /** These are the options for the assistant's voice. */
  voice?: AssistantVoice;
  /** This sets whether the assistant should detect voicemail. Defaults to true. */
  voicemailDetectionEnabled?: boolean;
  /** These are the AMD messages from Twilio that are considered as voicemail. Default is ['machine_end_beep', 'machine_end_silence', 'machine_end_other']. */
  voicemailDetectionTypes?: AssistantVoicemailDetectionTypesItem[];
  /**
   * This is the message that the assistant will say if the call is forwarded to voicemail.

If unspecified, it will hang up.
   * @maxLength 1000
   */
  voicemailMessage?: string;
}

export type CreateAssistantDTOVoicemailDetectionTypesItem = typeof CreateAssistantDTOVoicemailDetectionTypesItem[keyof typeof CreateAssistantDTOVoicemailDetectionTypesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateAssistantDTOVoicemailDetectionTypesItem = {
  machine_start: 'machine_start',
  human: 'human',
  fax: 'fax',
  unknown: 'unknown',
  machine_end_beep: 'machine_end_beep',
  machine_end_silence: 'machine_end_silence',
  machine_end_other: 'machine_end_other',
} as const;

/**
 * These are the options for the assistant's voice.
 */
export type CreateAssistantDTOVoice = AzureVoice | ElevenLabsVoice | PlayHTVoice | RimeAIVoice | DeepgramVoice | OpenAIVoice | LMNTVoice | NeetsVoice;

/**
 * These are the options for the assistant's transcriber.
 */
export type CreateAssistantDTOTranscriber = DeepgramTranscriber | TalkscriberTranscriber;

export type CreateAssistantDTOServerMessagesItem = typeof CreateAssistantDTOServerMessagesItem[keyof typeof CreateAssistantDTOServerMessagesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateAssistantDTOServerMessagesItem = {
  'status-update': 'status-update',
  transcript: 'transcript',
  hang: 'hang',
  'function-call': 'function-call',
  'tool-calls': 'tool-calls',
  'end-of-call-report': 'end-of-call-report',
  'conversation-update': 'conversation-update',
  'phone-call-control': 'phone-call-control',
  'model-output': 'model-output',
  'voice-input': 'voice-input',
} as const;

/**
 * These are the options for the assistant's LLM.
 */
export type CreateAssistantDTOModel = OpenAIModel | TogetherAIModel | AnyscaleModel | OpenRouterModel | PerplexityAIModel | DeepInfraModel | GroqModel | AnthropicModel | CustomLLMModel;

/**
 * This is the metadata associated with the call.
 */
export type CreateAssistantDTOMetadata = { [key: string]: any };

/**
 * This is the mode for the first message. Default is 'assistant-speaks-first'.

Specify 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
 */
export type CreateAssistantDTOFirstMessageMode = typeof CreateAssistantDTOFirstMessageMode[keyof typeof CreateAssistantDTOFirstMessageMode];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateAssistantDTOFirstMessageMode = {
  'assistant-speaks-first': 'assistant-speaks-first',
  'assistant-waits-for-user': 'assistant-waits-for-user',
} as const;

export type CreateAssistantDTOClientMessagesItem = typeof CreateAssistantDTOClientMessagesItem[keyof typeof CreateAssistantDTOClientMessagesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateAssistantDTOClientMessagesItem = {
  'status-update': 'status-update',
  'speech-update': 'speech-update',
  transcript: 'transcript',
  hang: 'hang',
  'function-call': 'function-call',
  'tool-calls': 'tool-calls',
  metadata: 'metadata',
  'conversation-update': 'conversation-update',
  'model-output': 'model-output',
  'voice-input': 'voice-input',
} as const;

/**
 * This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
 */
export type CreateAssistantDTOBackgroundSound = typeof CreateAssistantDTOBackgroundSound[keyof typeof CreateAssistantDTOBackgroundSound];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CreateAssistantDTOBackgroundSound = {
  off: 'off',
  office: 'office',
} as const;

export interface CreateAssistantDTO {
  /** backchanneling is the bot say while listening like 'mhmm', 'ahem' etc. this make the conversation sounds natural. Default True */
  backchannelingEnabled?: boolean;
  /** This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'. */
  backgroundSound?: CreateAssistantDTOBackgroundSound;
  /** These are the messages that will be sent to the Client SDKs. Default is ['transcript', 'hang', 'tool-calls', 'speech-update', 'metadata', 'conversation-update'] */
  clientMessages?: CreateAssistantDTOClientMessagesItem[];
  /** This sets whether the assistant can dial digits on the keypad. Defaults to false. */
  dialKeypadFunctionEnabled?: boolean;
  /** This sets whether the assistant will be able to hang up the call. Defaults to false. */
  endCallFunctionEnabled?: boolean;
  /**
   * This is the message that the assistant will say if it ends the call.

If unspecified, it will hang up without saying anything.
   * @maxLength 400
   */
  endCallMessage?: string;
  /** This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive. */
  endCallPhrases?: string[];
  /**
   * This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).

If unspecified, assistant will wait for user to speak and use the model to respond once they speak.
   * @maxLength 1000
   */
  firstMessage?: string;
  /** This is the mode for the first message. Default is 'assistant-speaks-first'.

Specify 'assistant-waits-for-user' to have the assistant wait for the user to speak first. */
  firstMessageMode?: CreateAssistantDTOFirstMessageMode;
  forwardingPhoneNumber?: string;
  /** When this is enabled, no logs, recordings, or transcriptions will be stored. At the end of the call, you will still receive an end-of-call-report message to store on your server. Defaults to false. */
  hipaaEnabled?: boolean;
  /**
   * The minimum number of seconds to wait after punctuation before sending a request to the LLM. Defaults to 0.1.
   * @minimum 0
   * @maximum 3
   */
  llmRequestDelaySeconds?: number;
  /**
   * This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
   * @minimum 10
   * @maximum 3600
   */
  maxDurationSeconds?: number;
  /** This is the metadata associated with the call. */
  metadata?: CreateAssistantDTOMetadata;
  /** These are the options for the assistant's LLM. */
  model?: CreateAssistantDTOModel;
  /**
   * This is the name of the assistant.
This is only required to transfer calls between assistants.
   * @maxLength 100
   */
  name?: string;
  /**
   * The number of words to wait for before interrupting the assistant. Words like "stop", "actually", "no", etc. will always interrupt immediately regardless of this value. Words like "okay", "yeah", "right" will never interrupt. Defaults to 1.
   * @minimum 1
   * @maximum 10
   */
  numWordsToInterruptAssistant?: number;
  /** This sets whether the assistant's calls are recorded. Defaults to true. */
  recordingEnabled?: boolean;
  /**
   * The minimum number of seconds after user speech to wait before the assistant starts speaking. Defaults to 0.4.
   * @minimum 0
   * @maximum 5
   */
  responseDelaySeconds?: number;
  /** These are the messages that will be sent to your Server URL. Default is ['end-of-call-report', 'status-update', 'hang', 'tool-calls'] */
  serverMessages?: CreateAssistantDTOServerMessagesItem[];
  /** This is the URL Vapi will communicate with via HTTP GET and POST Requests. This is used for retrieving context, function calling, and end-of-call reports.

All requests will be sent with the call object among other things relevant to that message. You can find more details in the Server URL documentation.

This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: function.serverUrl > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl */
  serverUrl?: string;
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.

Same precedence logic as serverUrl. */
  serverUrlSecret?: string;
  /**
   * How many seconds of silence to wait before ending the call. Defaults to 30.
   * @minimum 10
   * @maximum 600
   */
  silenceTimeoutSeconds?: number;
  /**
   * This is the prompt that's used to summarize the call at the end.

Default is 'You are an expert note-taker. You will be given a transcript of a conversation. Please summarize the conversation in 4-5 sentences if applicable.'

Set to '' or 'off' to disable post-call summarization.
   * @maxLength 2000
   */
  summaryPrompt?: string;
  /** These are the options for the assistant's transcriber. */
  transcriber?: CreateAssistantDTOTranscriber;
  /** These are the options for the assistant's voice. */
  voice?: CreateAssistantDTOVoice;
  /** This sets whether the assistant should detect voicemail. Defaults to true. */
  voicemailDetectionEnabled?: boolean;
  /** These are the AMD messages from Twilio that are considered as voicemail. Default is ['machine_end_beep', 'machine_end_silence', 'machine_end_other']. */
  voicemailDetectionTypes?: CreateAssistantDTOVoicemailDetectionTypesItem[];
  /**
   * This is the message that the assistant will say if the call is forwarded to voicemail.

If unspecified, it will hang up.
   * @maxLength 1000
   */
  voicemailMessage?: string;
}

/**
 * This is the provider-specific ID that will be used.
 */
export type NeetsVoiceVoiceId = 'lily' | 'daniel' | string;

/**
 * This is the voice provider that will be used.
 */
export type NeetsVoiceProvider = typeof NeetsVoiceProvider[keyof typeof NeetsVoiceProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const NeetsVoiceProvider = {
  neets: 'neets',
} as const;

export type NeetsVoiceInputPunctuationBoundariesItem = typeof NeetsVoiceInputPunctuationBoundariesItem[keyof typeof NeetsVoiceInputPunctuationBoundariesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const NeetsVoiceInputPunctuationBoundariesItem = {
  '„ÄÇ': '„ÄÇ',
  'Ôºå': 'Ôºå',
  '.': '.',
  '!': '!',
  '?': '?',
  ';': ';',
  ')': ')',
  'ÿå': 'ÿå',
  '€î': '€î',
  '‡•§': '‡•§',
  '‡••': '‡••',
  '|': '|',
  '||': '||',
  ',': ',',
  ':': ':',
} as const;

export interface NeetsVoice {
  /** This determines whether fillers are injected into the Model output before inputting it into the Voice provider. Defaults to false. */
  fillerInjectionEnabled?: boolean;
  /**
   * This is the minimum number of characters that will be passed to the voice provider. This helps decides the minimum chunk size that is sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to 30.
   * @minimum 1
   * @maximum 80
   */
  inputMinCharacters?: number;
  /** This determines whether LLM output is preprocessed before being sent to the voice provider. This includes things like giving better hints to the voice provider on how to pronounce complex structured text like phone numbers, emails and addresses. This might add latency as it waits for the LLM to output a full chunk before sending it to the voice provider. Defaults to true. */
  inputPreprocessingEnabled?: boolean;
  /** These are the punctuations that are considered valid boundaries / "delimiters". This helps decides the chunks that are sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to ['„ÄÇ', 'Ôºå', '.', '!', '?', ';', ')', 'ÿå', '€î', '‡•§', '‡••', '|', '||', ',', ':']. */
  inputPunctuationBoundaries?: NeetsVoiceInputPunctuationBoundariesItem[];
  /** This is the voice provider that will be used. */
  provider: NeetsVoiceProvider;
  /** This is the provider-specific ID that will be used. */
  voiceId: NeetsVoiceVoiceId;
}

/**
 * This is the provider-specific ID that will be used.
 */
export type LMNTVoiceVoiceId = 'lily' | 'daniel' | string;

/**
 * This is the voice provider that will be used.
 */
export type LMNTVoiceProvider = typeof LMNTVoiceProvider[keyof typeof LMNTVoiceProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const LMNTVoiceProvider = {
  lmnt: 'lmnt',
} as const;

export type LMNTVoiceInputPunctuationBoundariesItem = typeof LMNTVoiceInputPunctuationBoundariesItem[keyof typeof LMNTVoiceInputPunctuationBoundariesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const LMNTVoiceInputPunctuationBoundariesItem = {
  '„ÄÇ': '„ÄÇ',
  'Ôºå': 'Ôºå',
  '.': '.',
  '!': '!',
  '?': '?',
  ';': ';',
  ')': ')',
  'ÿå': 'ÿå',
  '€î': '€î',
  '‡•§': '‡•§',
  '‡••': '‡••',
  '|': '|',
  '||': '||',
  ',': ',',
  ':': ':',
} as const;

export interface LMNTVoice {
  /** This determines whether fillers are injected into the Model output before inputting it into the Voice provider. Defaults to false. */
  fillerInjectionEnabled?: boolean;
  /**
   * This is the minimum number of characters that will be passed to the voice provider. This helps decides the minimum chunk size that is sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to 30.
   * @minimum 1
   * @maximum 80
   */
  inputMinCharacters?: number;
  /** This determines whether LLM output is preprocessed before being sent to the voice provider. This includes things like giving better hints to the voice provider on how to pronounce complex structured text like phone numbers, emails and addresses. This might add latency as it waits for the LLM to output a full chunk before sending it to the voice provider. Defaults to true. */
  inputPreprocessingEnabled?: boolean;
  /** These are the punctuations that are considered valid boundaries / "delimiters". This helps decides the chunks that are sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to ['„ÄÇ', 'Ôºå', '.', '!', '?', ';', ')', 'ÿå', '€î', '‡•§', '‡••', '|', '||', ',', ':']. */
  inputPunctuationBoundaries?: LMNTVoiceInputPunctuationBoundariesItem[];
  /** This is the voice provider that will be used. */
  provider: LMNTVoiceProvider;
  /**
   * This is the speed multiplier that will be used.
   * @minimum 0.25
   * @maximum 2
   */
  speed?: number;
  /** This is the provider-specific ID that will be used. */
  voiceId: LMNTVoiceVoiceId;
}

/**
 * This is the provider-specific ID that will be used.
 */
export type AzureVoiceVoiceId = 'andrew' | 'brian' | 'emma' | string;

/**
 * This is the voice provider that will be used.
 */
export type AzureVoiceProvider = typeof AzureVoiceProvider[keyof typeof AzureVoiceProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AzureVoiceProvider = {
  azure: 'azure',
} as const;

export type AzureVoiceInputPunctuationBoundariesItem = typeof AzureVoiceInputPunctuationBoundariesItem[keyof typeof AzureVoiceInputPunctuationBoundariesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AzureVoiceInputPunctuationBoundariesItem = {
  '„ÄÇ': '„ÄÇ',
  'Ôºå': 'Ôºå',
  '.': '.',
  '!': '!',
  '?': '?',
  ';': ';',
  ')': ')',
  'ÿå': 'ÿå',
  '€î': '€î',
  '‡•§': '‡•§',
  '‡••': '‡••',
  '|': '|',
  '||': '||',
  ',': ',',
  ':': ':',
} as const;

export interface AzureVoice {
  /** This determines whether fillers are injected into the Model output before inputting it into the Voice provider. Defaults to false. */
  fillerInjectionEnabled?: boolean;
  /**
   * This is the minimum number of characters that will be passed to the voice provider. This helps decides the minimum chunk size that is sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to 30.
   * @minimum 1
   * @maximum 80
   */
  inputMinCharacters?: number;
  /** This determines whether LLM output is preprocessed before being sent to the voice provider. This includes things like giving better hints to the voice provider on how to pronounce complex structured text like phone numbers, emails and addresses. This might add latency as it waits for the LLM to output a full chunk before sending it to the voice provider. Defaults to true. */
  inputPreprocessingEnabled?: boolean;
  /** These are the punctuations that are considered valid boundaries / "delimiters". This helps decides the chunks that are sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to ['„ÄÇ', 'Ôºå', '.', '!', '?', ';', ')', 'ÿå', '€î', '‡•§', '‡••', '|', '||', ',', ':']. */
  inputPunctuationBoundaries?: AzureVoiceInputPunctuationBoundariesItem[];
  /** This is the voice provider that will be used. */
  provider: AzureVoiceProvider;
  /**
   * This is the speed multiplier that will be used.
   * @minimum 0.5
   * @maximum 2
   */
  speed?: number;
  /** This is the provider-specific ID that will be used. */
  voiceId: AzureVoiceVoiceId;
}

/**
 * This is the provider-specific ID that will be used.
 */
export type OpenAIVoiceVoiceId = typeof OpenAIVoiceVoiceId[keyof typeof OpenAIVoiceVoiceId];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenAIVoiceVoiceId = {
  alloy: 'alloy',
  echo: 'echo',
  fable: 'fable',
  onyx: 'onyx',
  nova: 'nova',
  shimmer: 'shimmer',
} as const;

/**
 * This is the voice provider that will be used.
 */
export type OpenAIVoiceProvider = typeof OpenAIVoiceProvider[keyof typeof OpenAIVoiceProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenAIVoiceProvider = {
  openai: 'openai',
} as const;

export type OpenAIVoiceInputPunctuationBoundariesItem = typeof OpenAIVoiceInputPunctuationBoundariesItem[keyof typeof OpenAIVoiceInputPunctuationBoundariesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenAIVoiceInputPunctuationBoundariesItem = {
  '„ÄÇ': '„ÄÇ',
  'Ôºå': 'Ôºå',
  '.': '.',
  '!': '!',
  '?': '?',
  ';': ';',
  ')': ')',
  'ÿå': 'ÿå',
  '€î': '€î',
  '‡•§': '‡•§',
  '‡••': '‡••',
  '|': '|',
  '||': '||',
  ',': ',',
  ':': ':',
} as const;

export interface OpenAIVoice {
  /** This determines whether fillers are injected into the Model output before inputting it into the Voice provider. Defaults to false. */
  fillerInjectionEnabled?: boolean;
  /**
   * This is the minimum number of characters that will be passed to the voice provider. This helps decides the minimum chunk size that is sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to 30.
   * @minimum 1
   * @maximum 80
   */
  inputMinCharacters?: number;
  /** This determines whether LLM output is preprocessed before being sent to the voice provider. This includes things like giving better hints to the voice provider on how to pronounce complex structured text like phone numbers, emails and addresses. This might add latency as it waits for the LLM to output a full chunk before sending it to the voice provider. Defaults to true. */
  inputPreprocessingEnabled?: boolean;
  /** These are the punctuations that are considered valid boundaries / "delimiters". This helps decides the chunks that are sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to ['„ÄÇ', 'Ôºå', '.', '!', '?', ';', ')', 'ÿå', '€î', '‡•§', '‡••', '|', '||', ',', ':']. */
  inputPunctuationBoundaries?: OpenAIVoiceInputPunctuationBoundariesItem[];
  /** This is the voice provider that will be used. */
  provider: OpenAIVoiceProvider;
  /**
   * This is the speed multiplier that will be used.
   * @minimum 0.25
   * @maximum 4
   */
  speed?: number;
  /** This is the provider-specific ID that will be used. */
  voiceId: OpenAIVoiceVoiceId;
}

/**
 * This is the provider-specific ID that will be used.
 */
export type RimeAIVoiceVoiceId = 'marsh' | 'bayou' | 'creek' | 'brook' | 'flower' | 'spore' | 'glacier' | 'gulch' | 'alpine' | 'cove' | 'lagoon' | 'tundra' | 'steppe' | 'mesa' | 'grove' | 'rainforest' | 'moraine' | 'wildflower' | 'peak' | 'boulder' | 'abbie' | 'allison' | 'ally' | 'alona' | 'amber' | 'ana' | 'antoine' | 'armon' | 'brenda' | 'brittany' | 'carol' | 'colin' | 'courtney' | 'elena' | 'elliot' | 'eva' | 'geoff' | 'gerald' | 'hank' | 'helen' | 'hera' | 'jen' | 'joe' | 'joy' | 'juan' | 'kendra' | 'kendrick' | 'kenneth' | 'kevin' | 'kris' | 'linda' | 'madison' | 'marge' | 'marina' | 'marissa' | 'marta' | 'maya' | 'nicholas' | 'nyles' | 'phil' | 'reba' | 'rex' | 'rick' | 'ritu' | 'rob' | 'rodney' | 'rohan' | 'rosco' | 'samantha' | 'sandy' | 'selena' | 'seth' | 'sharon' | 'stan' | 'tamra' | 'tanya' | 'tibur' | 'tj' | 'tyler' | 'viv' | 'yadira' | string;

/**
 * This is the voice provider that will be used.
 */
export type RimeAIVoiceProvider = typeof RimeAIVoiceProvider[keyof typeof RimeAIVoiceProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const RimeAIVoiceProvider = {
  'rime-ai': 'rime-ai',
} as const;

export type RimeAIVoiceInputPunctuationBoundariesItem = typeof RimeAIVoiceInputPunctuationBoundariesItem[keyof typeof RimeAIVoiceInputPunctuationBoundariesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const RimeAIVoiceInputPunctuationBoundariesItem = {
  '„ÄÇ': '„ÄÇ',
  'Ôºå': 'Ôºå',
  '.': '.',
  '!': '!',
  '?': '?',
  ';': ';',
  ')': ')',
  'ÿå': 'ÿå',
  '€î': '€î',
  '‡•§': '‡•§',
  '‡••': '‡••',
  '|': '|',
  '||': '||',
  ',': ',',
  ':': ':',
} as const;

export interface RimeAIVoice {
  /** This determines whether fillers are injected into the Model output before inputting it into the Voice provider. Defaults to false. */
  fillerInjectionEnabled?: boolean;
  /**
   * This is the minimum number of characters that will be passed to the voice provider. This helps decides the minimum chunk size that is sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to 30.
   * @minimum 1
   * @maximum 80
   */
  inputMinCharacters?: number;
  /** This determines whether LLM output is preprocessed before being sent to the voice provider. This includes things like giving better hints to the voice provider on how to pronounce complex structured text like phone numbers, emails and addresses. This might add latency as it waits for the LLM to output a full chunk before sending it to the voice provider. Defaults to true. */
  inputPreprocessingEnabled?: boolean;
  /** These are the punctuations that are considered valid boundaries / "delimiters". This helps decides the chunks that are sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to ['„ÄÇ', 'Ôºå', '.', '!', '?', ';', ')', 'ÿå', '€î', '‡•§', '‡••', '|', '||', ',', ':']. */
  inputPunctuationBoundaries?: RimeAIVoiceInputPunctuationBoundariesItem[];
  /** This is the voice provider that will be used. */
  provider: RimeAIVoiceProvider;
  /** @minimum 0.1 */
  speed?: number;
  /** This is the provider-specific ID that will be used. */
  voiceId: RimeAIVoiceVoiceId;
}

/**
 * This is the provider-specific ID that will be used.
 */
export type PlayHTVoiceVoiceId = 'jennifer' | 'melissa' | 'will' | 'chris' | 'matt' | 'jack' | 'ruby' | 'davis' | 'donna' | 'michael' | string;

/**
 * This is the voice provider that will be used.
 */
export type PlayHTVoiceProvider = typeof PlayHTVoiceProvider[keyof typeof PlayHTVoiceProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const PlayHTVoiceProvider = {
  playht: 'playht',
} as const;

export type PlayHTVoiceInputPunctuationBoundariesItem = typeof PlayHTVoiceInputPunctuationBoundariesItem[keyof typeof PlayHTVoiceInputPunctuationBoundariesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const PlayHTVoiceInputPunctuationBoundariesItem = {
  '„ÄÇ': '„ÄÇ',
  'Ôºå': 'Ôºå',
  '.': '.',
  '!': '!',
  '?': '?',
  ';': ';',
  ')': ')',
  'ÿå': 'ÿå',
  '€î': '€î',
  '‡•§': '‡•§',
  '‡••': '‡••',
  '|': '|',
  '||': '||',
  ',': ',',
  ':': ':',
} as const;

/**
 * An emotion to be applied to the speech.
 */
export type PlayHTVoiceEmotion = typeof PlayHTVoiceEmotion[keyof typeof PlayHTVoiceEmotion];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const PlayHTVoiceEmotion = {
  female_happy: 'female_happy',
  female_sad: 'female_sad',
  female_angry: 'female_angry',
  female_fearful: 'female_fearful',
  female_disgust: 'female_disgust',
  female_surprised: 'female_surprised',
  male_happy: 'male_happy',
  male_sad: 'male_sad',
  male_angry: 'male_angry',
  male_fearful: 'male_fearful',
  male_disgust: 'male_disgust',
  male_surprised: 'male_surprised',
} as const;

export interface PlayHTVoice {
  /** An emotion to be applied to the speech. */
  emotion?: PlayHTVoiceEmotion;
  /** This determines whether fillers are injected into the Model output before inputting it into the Voice provider. Defaults to false. */
  fillerInjectionEnabled?: boolean;
  /**
   * This is the minimum number of characters that will be passed to the voice provider. This helps decides the minimum chunk size that is sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to 30.
   * @minimum 1
   * @maximum 80
   */
  inputMinCharacters?: number;
  /** This determines whether LLM output is preprocessed before being sent to the voice provider. This includes things like giving better hints to the voice provider on how to pronounce complex structured text like phone numbers, emails and addresses. This might add latency as it waits for the LLM to output a full chunk before sending it to the voice provider. Defaults to true. */
  inputPreprocessingEnabled?: boolean;
  /** These are the punctuations that are considered valid boundaries / "delimiters". This helps decides the chunks that are sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to ['„ÄÇ', 'Ôºå', '.', '!', '?', ';', ')', 'ÿå', '€î', '‡•§', '‡••', '|', '||', ',', ':']. */
  inputPunctuationBoundaries?: PlayHTVoiceInputPunctuationBoundariesItem[];
  /** This is the voice provider that will be used. */
  provider: PlayHTVoiceProvider;
  /**
   * This is the speed multiplier that will be used.
   * @minimum 0.1
   * @maximum 5
   */
  speed?: number;
  /**
   * A number between 1 and 30. Use lower numbers to to reduce how strong your chosen emotion will be. Higher numbers will create a very emotional performance.
   * @minimum 1
   * @maximum 30
   */
  styleGuidance?: number;
  /**
   * A floating point number between 0, exclusive, and 2, inclusive. If equal to null or not provided, the model's default temperature will be used. The temperature parameter controls variance. Lower temperatures result in more predictable results, higher temperatures allow each run to vary more, so the voice may sound less like the baseline voice.
   * @minimum 0.1
   * @maximum 2
   */
  temperature?: number;
  /**
   * A number between 1 and 2. This number influences how closely the generated speech adheres to the input text. Use lower values to create more fluid speech, but with a higher chance of deviating from the input text. Higher numbers will make the generated speech more accurate to the input text, ensuring that the words spoken align closely with the provided text.
   * @minimum 1
   * @maximum 2
   */
  textGuidance?: number;
  /**
   * A number between 1 and 6. Use lower numbers to reduce how unique your chosen voice will be compared to other voices.
   * @minimum 1
   * @maximum 6
   */
  voiceGuidance?: number;
  /** This is the provider-specific ID that will be used. */
  voiceId: PlayHTVoiceVoiceId;
}

export type OpenAIModelToolsItem = TransferCallTool | FunctionTool | EndCallTool | DtmfTool;

/**
 * This is the provider that will be used for the model.
 */
export type OpenAIModelProvider = typeof OpenAIModelProvider[keyof typeof OpenAIModelProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenAIModelProvider = {
  openai: 'openai',
} as const;

/**
 * This is the OpenAI model that will be used.
 */
export type OpenAIModelModel = typeof OpenAIModelModel[keyof typeof OpenAIModelModel];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenAIModelModel = {
  'gpt-4o': 'gpt-4o',
  'gpt-4o-2024-05-13': 'gpt-4o-2024-05-13',
  'gpt-4-turbo': 'gpt-4-turbo',
  'gpt-4-turbo-2024-04-09': 'gpt-4-turbo-2024-04-09',
  'gpt-4-turbo-preview': 'gpt-4-turbo-preview',
  'gpt-4-0125-preview': 'gpt-4-0125-preview',
  'gpt-4-1106-preview': 'gpt-4-1106-preview',
  'gpt-4': 'gpt-4',
  'gpt-4-0613': 'gpt-4-0613',
  'gpt-35-turbo': 'gpt-3.5-turbo',
  'gpt-35-turbo-0125': 'gpt-3.5-turbo-0125',
  'gpt-35-turbo-1106': 'gpt-3.5-turbo-1106',
  'gpt-35-turbo-16k': 'gpt-3.5-turbo-16k',
  'gpt-35-turbo-0613': 'gpt-3.5-turbo-0613',
} as const;

export type OpenAIModelFallbackModelsItem = typeof OpenAIModelFallbackModelsItem[keyof typeof OpenAIModelFallbackModelsItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenAIModelFallbackModelsItem = {
  'gpt-4o': 'gpt-4o',
  'gpt-4o-2024-05-13': 'gpt-4o-2024-05-13',
  'gpt-4-turbo': 'gpt-4-turbo',
  'gpt-4-turbo-2024-04-09': 'gpt-4-turbo-2024-04-09',
  'gpt-4-turbo-preview': 'gpt-4-turbo-preview',
  'gpt-4-0125-preview': 'gpt-4-0125-preview',
  'gpt-4-1106-preview': 'gpt-4-1106-preview',
  'gpt-4': 'gpt-4',
  'gpt-4-0613': 'gpt-4-0613',
  'gpt-35-turbo': 'gpt-3.5-turbo',
  'gpt-35-turbo-0125': 'gpt-3.5-turbo-0125',
  'gpt-35-turbo-1106': 'gpt-3.5-turbo-1106',
  'gpt-35-turbo-16k': 'gpt-3.5-turbo-16k',
  'gpt-35-turbo-0613': 'gpt-3.5-turbo-0613',
} as const;

export interface OpenAIModel {
  /** This property handles whether we should detect user's emotion while they speak and send it as an additional info to model */
  emotionRecognitionEnabled?: boolean;
  /** These are the fallback models that will be used if the primary model fails. This shouldn't be specified unless you have a specific reason to do so. Vapi will automatically find the fastest fallbacks that make sense. */
  fallbackModels?: OpenAIModelFallbackModelsItem[];
  /** Configure your knowledge base by adding document uuids and provider. */
  knowledgeBase?: KnowledgeBase;
  /**
   * This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250.
   * @minimum 50
   * @maximum 1000
   */
  maxTokens?: number;
  /** This is the starting state for the conversation. */
  messages?: OpenAIMessage[];
  /** This is the OpenAI model that will be used. */
  model: OpenAIModelModel;
  /**
   * This sets how many turns at the start of the conversation to use gpt-3.5-turbo before switching to the primary model. Default is 0.
   * @minimum 0
   */
  numFastTurns?: number;
  /** This is the provider that will be used for the model. */
  provider: OpenAIModelProvider;
  semanticCachingEnabled?: boolean;
  /**
   * This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency.
   * @minimum 0
   * @maximum 2
   */
  temperature?: number;
  /** These are the tools functions that the assistant can execute during the call. */
  tools?: OpenAIModelToolsItem[];
}

/**
 * This is the provider-specific ID that will be used. Ensure the Voice is present in your 11Labs Voice Library.
 */
export type ElevenLabsVoiceVoiceId = 'burt' | 'marissa' | 'andrea' | 'sarah' | 'phillip' | 'steve' | 'joseph' | 'myra' | 'paula' | 'ryan' | 'drew' | 'paul' | 'mrb' | 'matilda' | 'mark' | string;

/**
 * This is the voice provider that will be used.
 */
export type ElevenLabsVoiceProvider = typeof ElevenLabsVoiceProvider[keyof typeof ElevenLabsVoiceProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ElevenLabsVoiceProvider = {
  '11labs': '11labs',
} as const;

/**
 * This is the model that will be used. Defaults to 'eleven_multilingual_v2' if transcriber.language is non-English, otherwise 'eleven_turbo_v2'.
 */
export type ElevenLabsVoiceModel = typeof ElevenLabsVoiceModel[keyof typeof ElevenLabsVoiceModel];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ElevenLabsVoiceModel = {
  eleven_multilingual_v2: 'eleven_multilingual_v2',
  eleven_turbo_v2: 'eleven_turbo_v2',
  eleven_monolingual_v1: 'eleven_monolingual_v1',
} as const;

export type ElevenLabsVoiceInputPunctuationBoundariesItem = typeof ElevenLabsVoiceInputPunctuationBoundariesItem[keyof typeof ElevenLabsVoiceInputPunctuationBoundariesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ElevenLabsVoiceInputPunctuationBoundariesItem = {
  '„ÄÇ': '„ÄÇ',
  'Ôºå': 'Ôºå',
  '.': '.',
  '!': '!',
  '?': '?',
  ';': ';',
  ')': ')',
  'ÿå': 'ÿå',
  '€î': '€î',
  '‡•§': '‡•§',
  '‡••': '‡••',
  '|': '|',
  '||': '||',
  ',': ',',
  ':': ':',
} as const;

export interface ElevenLabsVoice {
  /** Defines the use of https://elevenlabs.io/docs/speech-synthesis/prompting#pronunciation. Disabled by default. */
  enableSsmlParsing?: boolean;
  /** This determines whether fillers are injected into the Model output before inputting it into the Voice provider. Defaults to false. */
  fillerInjectionEnabled?: boolean;
  /**
   * This is the minimum number of characters that will be passed to the voice provider. This helps decides the minimum chunk size that is sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to 30.
   * @minimum 1
   * @maximum 80
   */
  inputMinCharacters?: number;
  /** This determines whether LLM output is preprocessed before being sent to the voice provider. This includes things like giving better hints to the voice provider on how to pronounce complex structured text like phone numbers, emails and addresses. This might add latency as it waits for the LLM to output a full chunk before sending it to the voice provider. Defaults to true. */
  inputPreprocessingEnabled?: boolean;
  /** These are the punctuations that are considered valid boundaries / "delimiters". This helps decides the chunks that are sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to ['„ÄÇ', 'Ôºå', '.', '!', '?', ';', ')', 'ÿå', '€î', '‡•§', '‡••', '|', '||', ',', ':']. */
  inputPunctuationBoundaries?: ElevenLabsVoiceInputPunctuationBoundariesItem[];
  /** This is the model that will be used. Defaults to 'eleven_multilingual_v2' if transcriber.language is non-English, otherwise 'eleven_turbo_v2'. */
  model?: ElevenLabsVoiceModel;
  /**
   * Defines the optimize streaming latency for voice settings. Defaults to 3.
   * @minimum 0
   * @maximum 4
   */
  optimizeStreamingLatency?: number;
  /** This is the voice provider that will be used. */
  provider: ElevenLabsVoiceProvider;
  /**
   * Defines the similarity boost for voice settings.
   * @minimum 0
   * @maximum 1
   */
  similarityBoost?: number;
  /**
   * Defines the stability for voice settings.
   * @minimum 0
   * @maximum 1
   */
  stability?: number;
  /**
   * Defines the style for voice settings.
   * @minimum 0
   * @maximum 1
   */
  style?: number;
  /** Defines the use speaker boost for voice settings. */
  useSpeakerBoost?: boolean;
  /** This is the provider-specific ID that will be used. Ensure the Voice is present in your 11Labs Voice Library. */
  voiceId: ElevenLabsVoiceVoiceId;
}

/**
 * This is the transcription provider that will be used.
 */
export type TalkscriberTranscriberProvider = typeof TalkscriberTranscriberProvider[keyof typeof TalkscriberTranscriberProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const TalkscriberTranscriberProvider = {
  talkscriber: 'talkscriber',
} as const;

/**
 * This is the model that will be used for the transcription.
 */
export type TalkscriberTranscriberModel = typeof TalkscriberTranscriberModel[keyof typeof TalkscriberTranscriberModel];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const TalkscriberTranscriberModel = {
  whisper: 'whisper',
} as const;

/**
 * This is the language that will be set for the transcription. The list of languages Whisper supports can be found here: https://github.com/openai/whisper/blob/main/whisper/tokenizer.py
 */
export type TalkscriberTranscriberLanguage = typeof TalkscriberTranscriberLanguage[keyof typeof TalkscriberTranscriberLanguage];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const TalkscriberTranscriberLanguage = {
  en: 'en',
  zh: 'zh',
  de: 'de',
  es: 'es',
  ru: 'ru',
  ko: 'ko',
  fr: 'fr',
  ja: 'ja',
  pt: 'pt',
  tr: 'tr',
  pl: 'pl',
  ca: 'ca',
  nl: 'nl',
  ar: 'ar',
  sv: 'sv',
  it: 'it',
  id: 'id',
  hi: 'hi',
  fi: 'fi',
  vi: 'vi',
  he: 'he',
  uk: 'uk',
  el: 'el',
  ms: 'ms',
  cs: 'cs',
  ro: 'ro',
  da: 'da',
  hu: 'hu',
  ta: 'ta',
  no: 'no',
  th: 'th',
  ur: 'ur',
  hr: 'hr',
  bg: 'bg',
  lt: 'lt',
  la: 'la',
  mi: 'mi',
  ml: 'ml',
  cy: 'cy',
  sk: 'sk',
  te: 'te',
  fa: 'fa',
  lv: 'lv',
  bn: 'bn',
  sr: 'sr',
  az: 'az',
  sl: 'sl',
  kn: 'kn',
  et: 'et',
  mk: 'mk',
  br: 'br',
  eu: 'eu',
  is: 'is',
  hy: 'hy',
  ne: 'ne',
  mn: 'mn',
  bs: 'bs',
  kk: 'kk',
  sq: 'sq',
  sw: 'sw',
  gl: 'gl',
  mr: 'mr',
  pa: 'pa',
  si: 'si',
  km: 'km',
  sn: 'sn',
  yo: 'yo',
  so: 'so',
  af: 'af',
  oc: 'oc',
  ka: 'ka',
  be: 'be',
  tg: 'tg',
  sd: 'sd',
  gu: 'gu',
  am: 'am',
  yi: 'yi',
  lo: 'lo',
  uz: 'uz',
  fo: 'fo',
  ht: 'ht',
  ps: 'ps',
  tk: 'tk',
  nn: 'nn',
  mt: 'mt',
  sa: 'sa',
  lb: 'lb',
  my: 'my',
  bo: 'bo',
  tl: 'tl',
  mg: 'mg',
  as: 'as',
  tt: 'tt',
  haw: 'haw',
  ln: 'ln',
  ha: 'ha',
  ba: 'ba',
  jw: 'jw',
  su: 'su',
  yue: 'yue',
} as const;

export interface TalkscriberTranscriber {
  /** This is the language that will be set for the transcription. The list of languages Whisper supports can be found here: https://github.com/openai/whisper/blob/main/whisper/tokenizer.py */
  language?: TalkscriberTranscriberLanguage;
  /** This is the model that will be used for the transcription. */
  model?: TalkscriberTranscriberModel;
  /** This is the transcription provider that will be used. */
  provider: TalkscriberTranscriberProvider;
}

/**
 * This is the transcription provider that will be used.
 */
export type DeepgramTranscriberProvider = typeof DeepgramTranscriberProvider[keyof typeof DeepgramTranscriberProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const DeepgramTranscriberProvider = {
  deepgram: 'deepgram',
} as const;

/**
 * This is the Deepgram model that will be used. A list of models can be found here: https://developers.deepgram.com/docs/models-languages-overview
 */
export type DeepgramTranscriberModel = 'nova-2' | 'nova-2-general' | 'nova-2-meeting' | 'nova-2-phonecall' | 'nova-2-finance' | 'nova-2-conversationalai' | 'nova-2-voicemail' | 'nova-2-video' | 'nova-2-medical' | 'nova-2-drivethru' | 'nova-2-automotive' | 'nova' | 'nova-general' | 'nova-phonecall' | 'nova-medical' | 'enhanced' | 'enhanced-general' | 'enhanced-meeting' | 'enhanced-phonecall' | 'enhanced-finance' | 'base' | 'base-general' | 'base-meeting' | 'base-phonecall' | 'base-finance' | 'base-conversationalai' | 'base-voicemail' | 'base-video' | string;

/**
 * This is the language that will be set for the transcription. The list of languages Deepgram supports can be found here: https://developers.deepgram.com/docs/models-languages-overview
 */
export type DeepgramTranscriberLanguage = typeof DeepgramTranscriberLanguage[keyof typeof DeepgramTranscriberLanguage];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const DeepgramTranscriberLanguage = {
  cs: 'cs',
  da: 'da',
  'da-DK': 'da-DK',
  'de-CH': 'de-CH',
  nl: 'nl',
  en: 'en',
  'en-US': 'en-US',
  'en-AU': 'en-AU',
  'en-GB': 'en-GB',
  'en-NZ': 'en-NZ',
  'en-IN': 'en-IN',
  'nl-BE': 'nl-BE',
  fr: 'fr',
  'fr-CA': 'fr-CA',
  de: 'de',
  el: 'el',
  hi: 'hi',
  'hi-Latn': 'hi-Latn',
  id: 'id',
  it: 'it',
  ja: 'ja',
  ko: 'ko',
  'ko-KR': 'ko-KR',
  no: 'no',
  pl: 'pl',
  pt: 'pt',
  'pt-BR': 'pt-BR',
  ru: 'ru',
  es: 'es',
  'es-419': 'es-419',
  sv: 'sv',
  'sv-SE': 'sv-SE',
  tr: 'tr',
  uk: 'uk',
  zh: 'zh',
  'zh-CN': 'zh-CN',
  'zh-TW': 'zh-TW',
} as const;

export interface DeepgramTranscriber {
  /** These keywords are passed to the transcription model to help it pick up use-case specific words. Anything that may not be a common word, like your company name, should be added here. */
  keywords?: string[];
  /** This is the language that will be set for the transcription. The list of languages Deepgram supports can be found here: https://developers.deepgram.com/docs/models-languages-overview */
  language?: DeepgramTranscriberLanguage;
  /** This is the Deepgram model that will be used. A list of models can be found here: https://developers.deepgram.com/docs/models-languages-overview */
  model?: DeepgramTranscriberModel;
  /** This is the transcription provider that will be used. */
  provider: DeepgramTranscriberProvider;
  /** This will be use smart format option provided by Deepgram. It's default disabled because it can sometimes format numbers as times sometimes but it's getting better. */
  smartFormat?: boolean;
}

export type AnthropicModelToolsItem = TransferCallTool | FunctionTool | EndCallTool | DtmfTool;

export type AnthropicModelProvider = typeof AnthropicModelProvider[keyof typeof AnthropicModelProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AnthropicModelProvider = {
  anthropic: 'anthropic',
} as const;

/**
 * This is the Anthropic/Claude models that will be used.
 */
export type AnthropicModelModel = typeof AnthropicModelModel[keyof typeof AnthropicModelModel];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AnthropicModelModel = {
  'claude-3-opus-20240229': 'claude-3-opus-20240229',
  'claude-3-sonnet-20240229': 'claude-3-sonnet-20240229',
  'claude-3-haiku-20240307': 'claude-3-haiku-20240307',
} as const;

export interface AnthropicModel {
  /** This property handles whether we should detect user's emotion while they speak and send it as an additional info to model */
  emotionRecognitionEnabled?: boolean;
  /** Configure your knowledge base by adding document uuids and provider. */
  knowledgeBase?: KnowledgeBase;
  /**
   * This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250.
   * @minimum 50
   * @maximum 1000
   */
  maxTokens?: number;
  /** This is the starting state for the conversation. */
  messages?: OpenAIMessage[];
  /** This is the Anthropic/Claude models that will be used. */
  model: AnthropicModelModel;
  provider: AnthropicModelProvider;
  /**
   * This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency.
   * @minimum 0
   * @maximum 2
   */
  temperature?: number;
  /** These are the tools functions that the assistant can execute during the call. */
  tools?: AnthropicModelToolsItem[];
}

export type GroqModelToolsItem = TransferCallTool | FunctionTool | EndCallTool | DtmfTool;

export type GroqModelProvider = typeof GroqModelProvider[keyof typeof GroqModelProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const GroqModelProvider = {
  groq: 'groq',
} as const;

/**
 * The key of the model from the custom provider. Ex. cognitivecomputations/dolphin-mixtral-8x7b
 */
export type GroqModelModel = typeof GroqModelModel[keyof typeof GroqModelModel];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const GroqModelModel = {
  'mixtral-8x7b-32768': 'mixtral-8x7b-32768',
  'llama3-8b-8192': 'llama3-8b-8192',
  'llama3-70b-8192': 'llama3-70b-8192',
} as const;

export interface GroqModel {
  /** This property handles whether we should detect user's emotion while they speak and send it as an additional info to model */
  emotionRecognitionEnabled?: boolean;
  /** Configure your knowledge base by adding document uuids and provider. */
  knowledgeBase?: KnowledgeBase;
  /**
   * This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250.
   * @minimum 50
   * @maximum 1000
   */
  maxTokens?: number;
  /** This is the starting state for the conversation. */
  messages?: OpenAIMessage[];
  /** The key of the model from the custom provider. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
  model: GroqModelModel;
  provider: GroqModelProvider;
  /**
   * This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency.
   * @minimum 0
   * @maximum 2
   */
  temperature?: number;
  /** These are the tools functions that the assistant can execute during the call. */
  tools?: GroqModelToolsItem[];
}

export type CustomLLMModelToolsItem = TransferCallTool | FunctionTool | EndCallTool | DtmfTool;

/**
 * This is the provider that will be used for the model. Any service, including your own server, that is compatible with the OpenAI API can be used.
 */
export type CustomLLMModelProvider = typeof CustomLLMModelProvider[keyof typeof CustomLLMModelProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const CustomLLMModelProvider = {
  'custom-llm': 'custom-llm',
} as const;

export interface CustomLLMModel {
  /** This property handles whether we should detect user's emotion while they speak and send it as an additional info to model */
  emotionRecognitionEnabled?: boolean;
  /** Configure your knowledge base by adding document uuids and provider. */
  knowledgeBase?: KnowledgeBase;
  /**
   * This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250.
   * @minimum 50
   * @maximum 1000
   */
  maxTokens?: number;
  /** This is the starting state for the conversation. */
  messages?: OpenAIMessage[];
  /** The key of the model from the custom provider. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
  model: string;
  /** This is the provider that will be used for the model. Any service, including your own server, that is compatible with the OpenAI API can be used. */
  provider: CustomLLMModelProvider;
  /**
   * This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency.
   * @minimum 0
   * @maximum 2
   */
  temperature?: number;
  /** These are the tools functions that the assistant can execute during the call. */
  tools?: CustomLLMModelToolsItem[];
  /** These is the URL we'll use for the OpenAI client's `baseURL`. Ex. https://openrouter.ai/api/v1 */
  url: string;
  /** This sets whether the call object is sent in requests to the custom provider. Default is true. */
  urlRequestMetadataEnabled: boolean;
}

export type DeepInfraModelToolsItem = TransferCallTool | FunctionTool | EndCallTool | DtmfTool;

export type DeepInfraModelProvider = typeof DeepInfraModelProvider[keyof typeof DeepInfraModelProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const DeepInfraModelProvider = {
  deepinfra: 'deepinfra',
} as const;

export interface DeepInfraModel {
  /** This property handles whether we should detect user's emotion while they speak and send it as an additional info to model */
  emotionRecognitionEnabled?: boolean;
  /** Configure your knowledge base by adding document uuids and provider. */
  knowledgeBase?: KnowledgeBase;
  /**
   * This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250.
   * @minimum 50
   * @maximum 1000
   */
  maxTokens?: number;
  /** This is the starting state for the conversation. */
  messages?: OpenAIMessage[];
  /** The key of the model from the custom provider. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
  model: string;
  provider: DeepInfraModelProvider;
  /**
   * This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency.
   * @minimum 0
   * @maximum 2
   */
  temperature?: number;
  /** These are the tools functions that the assistant can execute during the call. */
  tools?: DeepInfraModelToolsItem[];
}

export type PerplexityAIModelToolsItem = TransferCallTool | FunctionTool | EndCallTool | DtmfTool;

export type PerplexityAIModelProvider = typeof PerplexityAIModelProvider[keyof typeof PerplexityAIModelProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const PerplexityAIModelProvider = {
  'perplexity-ai': 'perplexity-ai',
} as const;

export interface PerplexityAIModel {
  /** This property handles whether we should detect user's emotion while they speak and send it as an additional info to model */
  emotionRecognitionEnabled?: boolean;
  /** Configure your knowledge base by adding document uuids and provider. */
  knowledgeBase?: KnowledgeBase;
  /**
   * This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250.
   * @minimum 50
   * @maximum 1000
   */
  maxTokens?: number;
  /** This is the starting state for the conversation. */
  messages?: OpenAIMessage[];
  /** The key of the model from the custom provider. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
  model: string;
  provider: PerplexityAIModelProvider;
  /**
   * This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency.
   * @minimum 0
   * @maximum 2
   */
  temperature?: number;
  /** These are the tools functions that the assistant can execute during the call. */
  tools?: PerplexityAIModelToolsItem[];
}

export type OpenRouterModelToolsItem = TransferCallTool | FunctionTool | EndCallTool | DtmfTool;

export type OpenRouterModelProvider = typeof OpenRouterModelProvider[keyof typeof OpenRouterModelProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenRouterModelProvider = {
  openrouter: 'openrouter',
} as const;

export interface OpenRouterModel {
  /** This property handles whether we should detect user's emotion while they speak and send it as an additional info to model */
  emotionRecognitionEnabled?: boolean;
  /** Configure your knowledge base by adding document uuids and provider. */
  knowledgeBase?: KnowledgeBase;
  /**
   * This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250.
   * @minimum 50
   * @maximum 1000
   */
  maxTokens?: number;
  /** This is the starting state for the conversation. */
  messages?: OpenAIMessage[];
  /** The key of the model from the custom provider. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
  model: string;
  provider: OpenRouterModelProvider;
  /**
   * This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency.
   * @minimum 0
   * @maximum 2
   */
  temperature?: number;
  /** These are the tools functions that the assistant can execute during the call. */
  tools?: OpenRouterModelToolsItem[];
}

export type AnyscaleModelToolsItem = TransferCallTool | FunctionTool | EndCallTool | DtmfTool;

export type AnyscaleModelProvider = typeof AnyscaleModelProvider[keyof typeof AnyscaleModelProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AnyscaleModelProvider = {
  anyscale: 'anyscale',
} as const;

export type TogetherAIModelToolsItem = TransferCallTool | FunctionTool | EndCallTool | DtmfTool;

export type TogetherAIModelProvider = typeof TogetherAIModelProvider[keyof typeof TogetherAIModelProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const TogetherAIModelProvider = {
  'together-ai': 'together-ai',
} as const;

export type KnowledgeBaseProvider = typeof KnowledgeBaseProvider[keyof typeof KnowledgeBaseProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const KnowledgeBaseProvider = {
  canonical: 'canonical',
} as const;

export interface KnowledgeBase {
  fileIds: string[];
  provider: KnowledgeBaseProvider;
  topK?: number;
}

export type OpenAIMessageRole = typeof OpenAIMessageRole[keyof typeof OpenAIMessageRole];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenAIMessageRole = {
  assistant: 'assistant',
  function: 'function',
  user: 'user',
  system: 'system',
  tool: 'tool',
} as const;

export interface OpenAIMessage {
  /** @nullable */
  content: string | null;
  role: OpenAIMessageRole;
}

export interface AnyscaleModel {
  /** This property handles whether we should detect user's emotion while they speak and send it as an additional info to model */
  emotionRecognitionEnabled?: boolean;
  /** Configure your knowledge base by adding document uuids and provider. */
  knowledgeBase?: KnowledgeBase;
  /**
   * This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250.
   * @minimum 50
   * @maximum 1000
   */
  maxTokens?: number;
  /** This is the starting state for the conversation. */
  messages?: OpenAIMessage[];
  /** The key of the model from the custom provider. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
  model: string;
  provider: AnyscaleModelProvider;
  /**
   * This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency.
   * @minimum 0
   * @maximum 2
   */
  temperature?: number;
  /** These are the tools functions that the assistant can execute during the call. */
  tools?: AnyscaleModelToolsItem[];
}

export interface TogetherAIModel {
  /** This property handles whether we should detect user's emotion while they speak and send it as an additional info to model */
  emotionRecognitionEnabled?: boolean;
  /** Configure your knowledge base by adding document uuids and provider. */
  knowledgeBase?: KnowledgeBase;
  /**
   * This is the max number of tokens that the assistant will be allowed to generate in each turn of the conversation. Default is 250.
   * @minimum 50
   * @maximum 1000
   */
  maxTokens?: number;
  /** This is the starting state for the conversation. */
  messages?: OpenAIMessage[];
  /** The key of the model from the custom provider. Ex. cognitivecomputations/dolphin-mixtral-8x7b */
  model: string;
  provider: TogetherAIModelProvider;
  /**
   * This is the temperature that will be used for calls. Default is 0 to leverage caching for lower latency.
   * @minimum 0
   * @maximum 2
   */
  temperature?: number;
  /** These are the tools functions that the assistant can execute during the call. */
  tools?: TogetherAIModelToolsItem[];
}

export type SipTransferDestinationType = typeof SipTransferDestinationType[keyof typeof SipTransferDestinationType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const SipTransferDestinationType = {
  sip: 'sip',
} as const;

export interface SipTransferDestination {
  /** This is the description of the destination, used by the AI to choose when and how to transfer the call. */
  description?: string;
  /** This is the message to say before transferring the call to the destination. */
  message?: string;
  /** This is the SIP URI to transfer the call to. */
  sipUri: string;
  type: SipTransferDestinationType;
}

export type PhoneNumberTransferDestinationType = typeof PhoneNumberTransferDestinationType[keyof typeof PhoneNumberTransferDestinationType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const PhoneNumberTransferDestinationType = {
  phoneNumber: 'phoneNumber',
} as const;

export interface PhoneNumberTransferDestination {
  /** This is the description of the destination, used by the AI to choose when and how to transfer the call. */
  description?: string;
  /** This is the message to say before transferring the call to the destination. */
  message?: string;
  /** This is the phone number to transfer the call to. */
  number: string;
  type: PhoneNumberTransferDestinationType;
}

export type AssistantTransferDestinationType = typeof AssistantTransferDestinationType[keyof typeof AssistantTransferDestinationType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const AssistantTransferDestinationType = {
  assistant: 'assistant',
} as const;

export interface AssistantTransferDestination {
  /** This is the assistant to transfer the call to. */
  assistantName: string;
  /** This is the description of the destination, used by the AI to choose when and how to transfer the call. */
  description?: string;
  /** This is the message to say before transferring the call to the destination. */
  message?: string;
  type: AssistantTransferDestinationType;
}

export type ToolMessageStartType = typeof ToolMessageStartType[keyof typeof ToolMessageStartType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ToolMessageStartType = {
  'request-start': 'request-start',
} as const;

export type ToolMessageFailedType = typeof ToolMessageFailedType[keyof typeof ToolMessageFailedType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ToolMessageFailedType = {
  'request-failed': 'request-failed',
} as const;

export type ToolMessageDelayedType = typeof ToolMessageDelayedType[keyof typeof ToolMessageDelayedType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ToolMessageDelayedType = {
  'request-response-delayed': 'request-response-delayed',
} as const;

export type ToolMessageCompleteType = typeof ToolMessageCompleteType[keyof typeof ToolMessageCompleteType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ToolMessageCompleteType = {
  'request-complete': 'request-complete',
} as const;

export interface ToolMessageComplete {
  conditions?: Condition[];
  content: string;
  type: ToolMessageCompleteType;
}

export type ConditionValue = { [key: string]: any };

export type ConditionOperator = typeof ConditionOperator[keyof typeof ConditionOperator];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const ConditionOperator = {
  eq: 'eq',
  neq: 'neq',
  gt: 'gt',
  gte: 'gte',
  lt: 'lt',
  lte: 'lte',
} as const;

export interface Condition {
  operator: ConditionOperator;
  param: string;
  value: ConditionValue;
}

export interface ToolMessageStart {
  conditions?: Condition[];
  content: string;
  type: ToolMessageStartType;
}

export interface ToolMessageFailed {
  conditions?: Condition[];
  content: string;
  type: ToolMessageFailedType;
}

export interface ToolMessageDelayed {
  conditions?: Condition[];
  content: string;
  /** The number of milliseconds to wait for the server response before saying this message. */
  timingMilliseconds?: number;
  type: ToolMessageDelayedType;
}

export type DtmfToolType = typeof DtmfToolType[keyof typeof DtmfToolType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const DtmfToolType = {
  dtmf: 'dtmf',
} as const;

export type DtmfToolMessagesItem = ToolMessageStart | ToolMessageComplete | ToolMessageFailed | ToolMessageDelayed;

export interface DtmfTool {
  /** This is optional but can be used to change the name and description of the default function provided to the model. */
  function: OpenAIFunction;
  messages?: DtmfToolMessagesItem[];
  type: DtmfToolType;
}

export type EndCallToolType = typeof EndCallToolType[keyof typeof EndCallToolType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const EndCallToolType = {
  endCall: 'endCall',
} as const;

export type EndCallToolMessagesItem = ToolMessageStart | ToolMessageComplete | ToolMessageFailed | ToolMessageDelayed;

export interface EndCallTool {
  /** This is optional but can be used to change the name and description of the default function provided to the model. */
  function: OpenAIFunction;
  /** These are the messages that will be spoken to the user as the tool is running. */
  messages?: EndCallToolMessagesItem[];
  type: EndCallToolType;
}

export type TransferCallToolType = typeof TransferCallToolType[keyof typeof TransferCallToolType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const TransferCallToolType = {
  transferCall: 'transferCall',
} as const;

export type TransferCallToolMessagesItem = ToolMessageStart | ToolMessageComplete | ToolMessageFailed | ToolMessageDelayed;

export type TransferCallToolDestinationsItem = { [key: string]: any };

export interface TransferCallTool {
  /** These are the destinations that the call can be transferred to. If no destinations are provided, server url will be used to get the transfer destination once the tool is called. */
  destinations?: TransferCallToolDestinationsItem[];
  /** This is optional but can be used to change the name and description of the default function provided to the model. */
  function: OpenAIFunction;
  messages?: TransferCallToolMessagesItem[];
  /** This is the server VAPI will hit when this tool is requested by the model if no destinations are provided.

All requests will be sent with the call object among other things. You can find more details in the Server URL documentation. */
  server?: Server;
  type: TransferCallToolType;
}

export type FunctionToolType = typeof FunctionToolType[keyof typeof FunctionToolType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const FunctionToolType = {
  function: 'function',
} as const;

export type FunctionToolMessagesItem = ToolMessageStart | ToolMessageComplete | ToolMessageFailed | ToolMessageDelayed;

export interface Server {
  /** This is the secret you can set that Vapi will send with every request to your server. Will be sent as a header called x-vapi-secret.

Same precedence logic as server. */
  secret?: string;
  /** API endpoint to send requests to. */
  url: string;
}

export interface OpenAIFunction {
  /**
   * This is the description of what the function does, used by the AI to choose when and how to call the function.
   * @maxLength 1000
   */
  description?: string;
  /**
   * This is the the name of the function to be called.

Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
   * @maxLength 64
   * @pattern /^[a-zA-Z0-9_-]{1,64}$/
   */
  name: string;
  /** These are the parameters the functions accepts, described as a JSON Schema object.

See the [OpenAI guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema) for documentation about the format.

Omitting parameters defines a function with an empty parameter list. */
  parameters?: OpenAIFunctionParameters;
}

export interface FunctionTool {
  /** Setting async: true will cause the function to be called asynchronously, meaning that the Assistant will not wait for the function to return before continuing. */
  async?: boolean;
  /** The function definition details for the function tool. */
  function: OpenAIFunction;
  /** These are the messages that will be spoken to the user as the tool is running. */
  messages?: FunctionToolMessagesItem[];
  /** This is the server VAPI will hit this tool is requested by the model.

All requests will be sent with the call object among other things. You can find more details in the Server URL documentation.

This overrides the serverUrl set on the org and the phoneNumber. Order of precedence: tool.server.url > assistant.serverUrl > phoneNumber.serverUrl > org.serverUrl */
  server?: Server;
  type: FunctionToolType;
}

/**
 * This must be set to 'object'. It instructs the model to return a JSON object containing the function call properties.
 */
export type OpenAIFunctionParametersType = typeof OpenAIFunctionParametersType[keyof typeof OpenAIFunctionParametersType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const OpenAIFunctionParametersType = {
  object: 'object',
} as const;

/**
 * This provides a description of the properties required by the function.
JSON Schema can be used to specify expectations for each property.
Refer to [this doc](https://ajv.js.org/json-schema.html#json-data-type) for a comprehensive guide on JSON Schema.
 */
export type OpenAIFunctionParametersProperties = {[key: string]: JsonSchema};

export interface OpenAIFunctionParameters {
  /** This provides a description of the properties required by the function.
JSON Schema can be used to specify expectations for each property.
Refer to [this doc](https://ajv.js.org/json-schema.html#json-data-type) for a comprehensive guide on JSON Schema. */
  properties: OpenAIFunctionParametersProperties;
  /** This specifies the properties that are required by the function. */
  required?: string[];
  /** This must be set to 'object'. It instructs the model to return a JSON object containing the function call properties. */
  type: OpenAIFunctionParametersType;
}

/**
 * This is of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaUnevaluatedProperties = { [key: string]: any };

export type JsonSchemaType = typeof JsonSchemaType[keyof typeof JsonSchemaType];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const JsonSchemaType = {
  string: 'string',
  number: 'number',
  integer: 'integer',
  boolean: 'boolean',
  array: 'array',
  object: 'object',
} as const;

/**
 * This is of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaThen = { [key: string]: any };

export type JsonSchemaRequiredItem = { [key: string]: any };

/**
 * This is of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaPropertyNames = { [key: string]: any };

/**
 * This is a map of string to JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaProperties = { [key: string]: any };

/**
 * This is an array of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaPrefixItems = { [key: string]: any };

/**
 * This is a map of string to JsonSchema objects. However, Swagger doesn't support circular references.
 */
export type JsonSchemaPatternProperties = { [key: string]: any };

/**
 * This is an array of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaOneOf = { [key: string]: any };

/**
 * This is of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaNot = { [key: string]: any };

/**
 * This is of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaItems = { [key: string]: any };

/**
 * This is of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaIf = { [key: string]: any };

export type JsonSchemaExamplesItem = { [key: string]: any };

export type JsonSchemaEnumItem = { [key: string]: any };

/**
 * This is of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaElse = { [key: string]: any };

/**
 * This is a map of string to JsonSchema objects. However, Swagger doesn't support circular references.
 */
export type JsonSchemaDependentSchemas = { [key: string]: any };

export type JsonSchemaDependentRequired = { [key: string]: any };

export type JsonSchemaDefault = { [key: string]: any };

/**
 * This is of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaContains = { [key: string]: any };

/**
 * This is of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaConst = { [key: string]: any };

/**
 * This is an array of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaAnyOf = { [key: string]: any };

/**
 * This is an array of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaAllOf = { [key: string]: any };

/**
 * This is of type JsonSchema. However, Swagger doesn't support circular references.
 */
export type JsonSchemaAdditionalProperties = { [key: string]: any };

export interface JsonSchema {
  $comment?: string;
  $id?: string;
  $ref?: string;
  $schema?: string;
  /** This is of type JsonSchema. However, Swagger doesn't support circular references. */
  additionalProperties?: JsonSchemaAdditionalProperties;
  /** This is an array of type JsonSchema. However, Swagger doesn't support circular references. */
  allOf?: JsonSchemaAllOf;
  /** This is an array of type JsonSchema. However, Swagger doesn't support circular references. */
  anyOf?: JsonSchemaAnyOf;
  /** This is of type JsonSchema. However, Swagger doesn't support circular references. */
  const?: JsonSchemaConst;
  /** This is of type JsonSchema. However, Swagger doesn't support circular references. */
  contains?: JsonSchemaContains;
  contentEncoding?: string;
  contentMediaType?: string;
  default?: JsonSchemaDefault;
  dependentRequired?: JsonSchemaDependentRequired;
  /** This is a map of string to JsonSchema objects. However, Swagger doesn't support circular references. */
  dependentSchemas?: JsonSchemaDependentSchemas;
  description?: string;
  /** This is of type JsonSchema. However, Swagger doesn't support circular references. */
  else?: JsonSchemaElse;
  enum?: JsonSchemaEnumItem[];
  examples?: JsonSchemaExamplesItem[];
  exclusiveMaximum?: number;
  exclusiveMinimum?: number;
  format?: string;
  /** This is of type JsonSchema. However, Swagger doesn't support circular references. */
  if?: JsonSchemaIf;
  /** This is of type JsonSchema. However, Swagger doesn't support circular references. */
  items?: JsonSchemaItems;
  maxContains?: number;
  maximum?: number;
  maxItems?: number;
  maxLength?: number;
  maxProperties?: number;
  minContains?: number;
  minimum?: number;
  /** @minimum 0 */
  minItems?: number;
  /** @minimum 0 */
  minLength?: number;
  /** @minimum 0 */
  minProperties?: number;
  multipleOf?: number;
  /** This is of type JsonSchema. However, Swagger doesn't support circular references. */
  not?: JsonSchemaNot;
  /** This is an array of type JsonSchema. However, Swagger doesn't support circular references. */
  oneOf?: JsonSchemaOneOf;
  pattern?: string;
  /** This is a map of string to JsonSchema objects. However, Swagger doesn't support circular references. */
  patternProperties?: JsonSchemaPatternProperties;
  /** This is an array of type JsonSchema. However, Swagger doesn't support circular references. */
  prefixItems?: JsonSchemaPrefixItems;
  /** This is a map of string to JsonSchema. However, Swagger doesn't support circular references. */
  properties?: JsonSchemaProperties;
  /** This is of type JsonSchema. However, Swagger doesn't support circular references. */
  propertyNames?: JsonSchemaPropertyNames;
  readOnly?: boolean;
  required?: JsonSchemaRequiredItem[];
  /** This is of type JsonSchema. However, Swagger doesn't support circular references. */
  then?: JsonSchemaThen;
  title?: string;
  type: JsonSchemaType;
  /** This is of type JsonSchema. However, Swagger doesn't support circular references. */
  unevaluatedProperties?: JsonSchemaUnevaluatedProperties;
  writeOnly?: boolean;
}

/**
 * This is the provider-specific ID that will be used.
 */
export type DeepgramVoiceVoiceId = 'asteria' | 'luna' | 'stella' | 'athena' | 'hera' | 'orion' | 'arcas' | 'perseus' | 'angus' | 'orpheus' | 'helios' | 'zeus' | string;

/**
 * This is the voice provider that will be used.
 */
export type DeepgramVoiceProvider = typeof DeepgramVoiceProvider[keyof typeof DeepgramVoiceProvider];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const DeepgramVoiceProvider = {
  deepgram: 'deepgram',
} as const;

export type DeepgramVoiceInputPunctuationBoundariesItem = typeof DeepgramVoiceInputPunctuationBoundariesItem[keyof typeof DeepgramVoiceInputPunctuationBoundariesItem];


// eslint-disable-next-line @typescript-eslint/no-redeclare
export const DeepgramVoiceInputPunctuationBoundariesItem = {
  '„ÄÇ': '„ÄÇ',
  'Ôºå': 'Ôºå',
  '.': '.',
  '!': '!',
  '?': '?',
  ';': ';',
  ')': ')',
  'ÿå': 'ÿå',
  '€î': '€î',
  '‡•§': '‡•§',
  '‡••': '‡••',
  '|': '|',
  '||': '||',
  ',': ',',
  ':': ':',
} as const;

export interface DeepgramVoice {
  /** This determines whether fillers are injected into the Model output before inputting it into the Voice provider. Defaults to false. */
  fillerInjectionEnabled?: boolean;
  /**
   * This is the minimum number of characters that will be passed to the voice provider. This helps decides the minimum chunk size that is sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to 30.
   * @minimum 1
   * @maximum 80
   */
  inputMinCharacters?: number;
  /** This determines whether LLM output is preprocessed before being sent to the voice provider. This includes things like giving better hints to the voice provider on how to pronounce complex structured text like phone numbers, emails and addresses. This might add latency as it waits for the LLM to output a full chunk before sending it to the voice provider. Defaults to true. */
  inputPreprocessingEnabled?: boolean;
  /** These are the punctuations that are considered valid boundaries / "delimiters". This helps decides the chunks that are sent to the voice provider for the voice generation as the LLM tokens are streaming in. Defaults to ['„ÄÇ', 'Ôºå', '.', '!', '?', ';', ')', 'ÿå', '€î', '‡•§', '‡••', '|', '||', ',', ':']. */
  inputPunctuationBoundaries?: DeepgramVoiceInputPunctuationBoundariesItem[];
  /** This is the voice provider that will be used. */
  provider: DeepgramVoiceProvider;
  /** This is the provider-specific ID that will be used. */
  voiceId: DeepgramVoiceVoiceId;
}





  /**
 * @summary Create Assistant
 */
export const assistantControllerCreate = <TData = AxiosResponse<Assistant>>(
    createAssistantDTO: CreateAssistantDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.post(
      `https://api.vapi.ai/assistant`,
      createAssistantDTO,options
    );
  }

/**
 * @summary List Assistants
 */
export const assistantControllerFindAll = <TData = AxiosResponse<Assistant[]>>(
    params?: AssistantControllerFindAllParams, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/assistant`,{
    ...options,
        params: {...params, ...options?.params},}
    );
  }

/**
 * @summary Get Assistant
 */
export const assistantControllerFindOne = <TData = AxiosResponse<Assistant>>(
    id: string, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/assistant/${id}`,options
    );
  }

/**
 * @summary Update Assistant
 */
export const assistantControllerUpdate = <TData = AxiosResponse<Assistant>>(
    id: string,
    updateAssistantDTO: UpdateAssistantDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.patch(
      `https://api.vapi.ai/assistant/${id}`,
      updateAssistantDTO,options
    );
  }

/**
 * @summary Replace Assistant
 */
export const assistantControllerReplace = <TData = AxiosResponse<Assistant>>(
    id: string,
    updateAssistantDTO: UpdateAssistantDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.put(
      `https://api.vapi.ai/assistant/${id}`,
      updateAssistantDTO,options
    );
  }

/**
 * @summary Delete Assistant
 */
export const assistantControllerRemove = <TData = AxiosResponse<Assistant>>(
    id: string, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.delete(
      `https://api.vapi.ai/assistant/${id}`,options
    );
  }

/**
 * @summary List Calls
 */
export const callControllerFindAll = <TData = AxiosResponse<Call[]>>(
    params?: CallControllerFindAllParams, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/call`,{
    ...options,
        params: {...params, ...options?.params},}
    );
  }

/**
 * @summary List Calls
 */
export const callControllerFindAllPaginated = <TData = AxiosResponse<CallPaginatedResponse>>(
    params?: CallControllerFindAllPaginatedParams, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/v2/call`,{
    ...options,
        params: {...params, ...options?.params},}
    );
  }

/**
 * @summary Get Call
 */
export const callControllerFindOne = <TData = AxiosResponse<Call>>(
    id: string, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/call/${id}`,options
    );
  }

/**
 * @summary Create Phone Call
 */
export const callControllerCreatePhoneCall = <TData = AxiosResponse<Call>>(
    createOutboundCallDTO: CreateOutboundCallDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.post(
      `https://api.vapi.ai/call/phone`,
      createOutboundCallDTO,options
    );
  }

/**
 * @summary Create Credential
 */
export const credentialControllerCreate = <TData = AxiosResponse<CredentialControllerCreate201>>(
    credentialControllerCreateBody: CredentialControllerCreateBody, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.post(
      `https://api.vapi.ai/credential`,
      credentialControllerCreateBody,options
    );
  }

/**
 * @summary List Credentials
 */
export const credentialControllerFindAll = <TData = AxiosResponse<CredentialControllerFindAll200Item[]>>(
    params?: CredentialControllerFindAllParams, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/credential`,{
    ...options,
        params: {...params, ...options?.params},}
    );
  }

/**
 * @summary Get Credential
 */
export const credentialControllerFindOne = <TData = AxiosResponse<CredentialControllerFindOne200>>(
    id: string, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/credential/${id}`,options
    );
  }

/**
 * @summary Update Credential
 */
export const credentialControllerUpdate = <TData = AxiosResponse<CredentialControllerUpdate200>>(
    id: string,
    credentialControllerUpdateBody: CredentialControllerUpdateBody, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.put(
      `https://api.vapi.ai/credential/${id}`,
      credentialControllerUpdateBody,options
    );
  }

/**
 * @summary Delete Credential
 */
export const credentialControllerRemove = <TData = AxiosResponse<CredentialControllerRemove200>>(
    id: string, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.delete(
      `https://api.vapi.ai/credential/${id}`,options
    );
  }

/**
 * @summary List Orgs
 */
export const orgControllerFindAll = <TData = AxiosResponse<Org[]>>(
     options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/org`,options
    );
  }

/**
 * @summary List Users
 */
export const orgControllerFindAllUsers = <TData = AxiosResponse<User[]>>(
     options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/org/user`,options
    );
  }

/**
 * @summary Get Org
 */
export const orgControllerFindOne = <TData = AxiosResponse<Org>>(
    id: string, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/org/${id}`,options
    );
  }

/**
 * @summary Update Org
 */
export const orgControllerUpdate = <TData = AxiosResponse<Org>>(
    id: string,
    updateOrgDTO: UpdateOrgDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.patch(
      `https://api.vapi.ai/org/${id}`,
      updateOrgDTO,options
    );
  }

/**
 * @summary Invite User
 */
export const orgControllerUserInvite = <TData = AxiosResponse<void | OrgControllerUserInvite201>>(
    inviteUserDTO: InviteUserDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.post(
      `https://api.vapi.ai/org/invite`,
      inviteUserDTO,options
    );
  }

/**
 * @summary Buy Phone Number
 */
export const phoneNumberControllerBuy = <TData = AxiosResponse<PhoneNumber>>(
    buyPhoneNumberDTO: BuyPhoneNumberDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.post(
      `https://api.vapi.ai/phone-number/buy`,
      buyPhoneNumberDTO,options
    );
  }

/**
 * @summary Import Twilio Number
 */
export const phoneNumberControllerImportTwilio = <TData = AxiosResponse<PhoneNumber>>(
    importTwilioPhoneNumberDTO: ImportTwilioPhoneNumberDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.post(
      `https://api.vapi.ai/phone-number/import/twilio`,
      importTwilioPhoneNumberDTO,options
    );
  }

/**
 * @summary Import Vonage Number
 */
export const phoneNumberControllerImportVonage = <TData = AxiosResponse<PhoneNumber>>(
    importVonagePhoneNumberDTO: ImportVonagePhoneNumberDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.post(
      `https://api.vapi.ai/phone-number/import/vonage`,
      importVonagePhoneNumberDTO,options
    );
  }

/**
 * @summary List Phone Numbers
 */
export const phoneNumberControllerFindAll = <TData = AxiosResponse<PhoneNumber[]>>(
    params?: PhoneNumberControllerFindAllParams, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/phone-number`,{
    ...options,
        params: {...params, ...options?.params},}
    );
  }

/**
 * @summary Get Phone Number
 */
export const phoneNumberControllerFindOne = <TData = AxiosResponse<PhoneNumber>>(
    id: string, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/phone-number/${id}`,options
    );
  }

/**
 * @summary Update Phone Number
 */
export const phoneNumberControllerUpdate = <TData = AxiosResponse<PhoneNumber>>(
    id: string,
    updatePhoneNumberDTO: UpdatePhoneNumberDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.patch(
      `https://api.vapi.ai/phone-number/${id}`,
      updatePhoneNumberDTO,options
    );
  }

/**
 * @summary Delete Phone Number
 */
export const phoneNumberControllerRemove = <TData = AxiosResponse<PhoneNumber>>(
    id: string, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.delete(
      `https://api.vapi.ai/phone-number/${id}`,options
    );
  }

/**
 * @summary List Metrics
 */
export const metricsControllerFindAll = <TData = AxiosResponse<Metrics[]>>(
    params?: MetricsControllerFindAllParams, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/metrics`,{
    ...options,
        params: {...params, ...options?.params},}
    );
  }

/**
 * @summary Get voices in Voice Library by Providers
 */
export const voiceLibraryControllerVoiceGetByProvider = <TData = AxiosResponse<VoiceLibrary[]>>(
    provider: '11labs' | 'playht' | 'rime-ai' | 'deepgram' | 'openai' | 'azure' | 'lmnt' | 'neets', options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/voice-library/${provider}`,options
    );
  }

/**
 * @summary Sync voices in Voice Library by Providers
 */
export const voiceLibraryControllerVoiceLibrarySyncByProvider = <TData = AxiosResponse<VoiceLibrary[] | VoiceLibraryControllerVoiceLibrarySyncByProvider201>>(
    provider: '11labs' | 'playht' | 'rime-ai' | 'deepgram' | 'openai' | 'azure' | 'lmnt' | 'neets', options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.post(
      `https://api.vapi.ai/voice-library/sync/${provider}`,undefined,options
    );
  }

/**
 * @summary Get Call Logs
 */
export const loggingControllerGetLogs = <TData = AxiosResponse<CallLogsPaginatedResponse>>(
    params: LoggingControllerGetLogsParams, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/log`,{
    ...options,
        params: {...params, ...options?.params},}
    );
  }

/**
 * @summary Upload File
 */
export const fileControllerCreate = <TData = AxiosResponse<File>>(
    createFileDTO: CreateFileDTO, options?: AxiosRequestConfig
 ): Promise<TData> => {const formData = new FormData();
formData.append('file', createFileDTO.file)

    return axios.post(
      `https://api.vapi.ai/file/upload`,
      formData,options
    );
  }

/**
 * @summary List Files
 */
export const fileControllerFindAll = <TData = AxiosResponse<File[]>>(
     options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/file`,options
    );
  }

/**
 * @summary Get File
 */
export const fileControllerFindOne = <TData = AxiosResponse<File>>(
    id: string, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.get(
      `https://api.vapi.ai/file/${id}`,options
    );
  }

/**
 * @summary Delete File
 */
export const fileControllerRemove = <TData = AxiosResponse<File>>(
    id: string, options?: AxiosRequestConfig
 ): Promise<TData> => {
    return axios.delete(
      `https://api.vapi.ai/file/${id}`,options
    );
  }

export type AssistantControllerCreateResult = AxiosResponse<Assistant>
export type AssistantControllerFindAllResult = AxiosResponse<Assistant[]>
export type AssistantControllerFindOneResult = AxiosResponse<Assistant>
export type AssistantControllerUpdateResult = AxiosResponse<Assistant>
export type AssistantControllerReplaceResult = AxiosResponse<Assistant>
export type AssistantControllerRemoveResult = AxiosResponse<Assistant>
export type CallControllerFindAllResult = AxiosResponse<Call[]>
export type CallControllerFindAllPaginatedResult = AxiosResponse<CallPaginatedResponse>
export type CallControllerFindOneResult = AxiosResponse<Call>
export type CallControllerCreatePhoneCallResult = AxiosResponse<Call>
export type CredentialControllerCreateResult = AxiosResponse<CredentialControllerCreate201>
export type CredentialControllerFindAllResult = AxiosResponse<CredentialControllerFindAll200Item[]>
export type CredentialControllerFindOneResult = AxiosResponse<CredentialControllerFindOne200>
export type CredentialControllerUpdateResult = AxiosResponse<CredentialControllerUpdate200>
export type CredentialControllerRemoveResult = AxiosResponse<CredentialControllerRemove200>
export type OrgControllerFindAllResult = AxiosResponse<Org[]>
export type OrgControllerFindAllUsersResult = AxiosResponse<User[]>
export type OrgControllerFindOneResult = AxiosResponse<Org>
export type OrgControllerUpdateResult = AxiosResponse<Org>
export type OrgControllerUserInviteResult = AxiosResponse<void | OrgControllerUserInvite201>
export type PhoneNumberControllerBuyResult = AxiosResponse<PhoneNumber>
export type PhoneNumberControllerImportTwilioResult = AxiosResponse<PhoneNumber>
export type PhoneNumberControllerImportVonageResult = AxiosResponse<PhoneNumber>
export type PhoneNumberControllerFindAllResult = AxiosResponse<PhoneNumber[]>
export type PhoneNumberControllerFindOneResult = AxiosResponse<PhoneNumber>
export type PhoneNumberControllerUpdateResult = AxiosResponse<PhoneNumber>
export type PhoneNumberControllerRemoveResult = AxiosResponse<PhoneNumber>
export type MetricsControllerFindAllResult = AxiosResponse<Metrics[]>
export type VoiceLibraryControllerVoiceGetByProviderResult = AxiosResponse<VoiceLibrary[]>
export type VoiceLibraryControllerVoiceLibrarySyncByProviderResult = AxiosResponse<VoiceLibrary[] | VoiceLibraryControllerVoiceLibrarySyncByProvider201>
export type LoggingControllerGetLogsResult = AxiosResponse<CallLogsPaginatedResponse>
export type FileControllerCreateResult = AxiosResponse<File>
export type FileControllerFindAllResult = AxiosResponse<File[]>
export type FileControllerFindOneResult = AxiosResponse<File>
export type FileControllerRemoveResult = AxiosResponse<File>
